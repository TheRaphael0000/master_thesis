{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook extensions\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Std Python Lib\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "# Requirements\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy.stats import wilcoxon\n",
    "from scipy.stats import linregress\n",
    "from scipy.stats import beta\n",
    "from adjustText import adjust_text\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Project\n",
    "import distances\n",
    "import compressions\n",
    "import s_curves\n",
    "\n",
    "from corpus import brunet\n",
    "from corpus import oxquarry\n",
    "from corpus import st_jean\n",
    "from corpus import pan16\n",
    "\n",
    "\n",
    "from rank_list_fusion import fusion_z_score\n",
    "from rank_list_fusion import fusion_regression_training\n",
    "from rank_list_fusion import fusion_regression_trainings\n",
    "from rank_list_fusion import fusion_regression\n",
    "\n",
    "from evaluate import evaluate_linking\n",
    "from evaluate import evaluate_clustering\n",
    "\n",
    "from linking import compute_links\n",
    "from linking import most_frequent_word\n",
    "\n",
    "from clustering import dist_thresh_logistic_regression\n",
    "from clustering import dist_thresh_two_beta\n",
    "from clustering import clustering_at_dist_thresh\n",
    "from clustering import silhouette_based_clustering\n",
    "from clustering import clustering_at_every_n_clusters\n",
    "from clustering import agglomerative_clustering\n",
    "from clustering import best_clustering\n",
    "\n",
    "from misc import sign_test\n",
    "from misc import first_letters_cut\n",
    "from misc import word_n_grams\n",
    "from misc import last_letters_cut\n",
    "from misc import sigmoid\n",
    "from misc import sigmoid_r\n",
    "from misc import compute_r\n",
    "from misc import normalize\n",
    "from misc import rank_list_distance\n",
    "from misc import dataset_infos\n",
    "from misc import sort_Y_and_distance_matrix\n",
    "from misc import subset_Y_and_distance_matrix\n",
    "from misc import distances_matrix_from_rank_list\n",
    "from misc import fit_beta\n",
    "from misc import find_two_beta_same_area\n",
    "from misc import features_from_rank_list\n",
    "from misc import labels_from_rank_list\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2, floatmode=\"fixed\", suppress=True)\n",
    "mpl.rcParams[\"figure.facecolor\"] = \"w\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Oxquarry\")\n",
    "info_oxquarry, X_token_oxquarry, Y_oxquarry = oxquarry.parse()\n",
    "print(\"Loading Brunet\")\n",
    "info_brunet, X_lemma_brunet, X_token_brunet, Y_brunet = brunet.parse()\n",
    "print(\"Loading St-Jean A\")\n",
    "info_st_jean_A, X_pos_st_jean_A, X_lemma_st_jean_A, X_token_st_jean_A, Y_st_jean_A = st_jean.parse_A()\n",
    "print(\"Loading St-Jean B\")\n",
    "info_st_jean_B, X_pos_st_jean_B, X_lemma_st_jean_B, X_token_st_jean_B, Y_st_jean_B = st_jean.parse_B()\n",
    "print(\"Loading St-Jean\")\n",
    "info_st_jean, X_pos_st_jean, X_lemma_st_jean, X_token_st_jean, Y_st_jean = st_jean.parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Name, Language, authors, texts, r, true_links, links, true_links_ratio, mean_length\")\n",
    "id, x, y = oxquarry.parse()\n",
    "print(\"Oxquarry EN\", *dataset_infos(x, y))\n",
    "id, x_lemma, x, y = brunet.parse()\n",
    "print(\"Brunet FR\", *dataset_infos(x, y))\n",
    "id, x_pos, x_lemma, x, y = st_jean.parse()\n",
    "print(\"St-Jean FR\", *dataset_infos(x, y))\n",
    "print(\"St-Jean A 001-100 FR\", *dataset_infos(x[:100], y[:100]))\n",
    "print(\"St-Jean B 101-200 FR\", *dataset_infos(x[100:], y[100:]))\n",
    "\n",
    "problems = pan16.parse_train()\n",
    "for (info, _, x, y) in problems:\n",
    "    print(f\"PAN16 {info['language']} {info['folder']}\", *dataset_infos(x, y))\n",
    "problems = pan16.parse_test()\n",
    "for (info, _, x, y) in problems:\n",
    "    print(f\"PAN16 {info['language']} {info['folder']}\", *dataset_infos(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MF Token and Lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mfw(mfw, n):\n",
    "    for i, (k, v) in enumerate(list(mfw.items())[0:n]):\n",
    "        print(f\"{i+1} & {k} & {v} \\\\\\\\\")\n",
    "\n",
    "\n",
    "features, mfw = most_frequent_word(X_token_st_jean, 500, z_score=False, lidstone_lambda=0.1, remove_hapax=True)\n",
    "print_mfw(mfw, 40)\n",
    "\n",
    "features, mfw = most_frequent_word(X_lemma_st_jean, 500, z_score=False, lidstone_lambda=0.1, remove_hapax=True)\n",
    "print_mfw(mfw, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfs = np.arange(250, 2000 + 1, 250)\n",
    "distances_ = distances.vector_distances\n",
    "\n",
    "def evaluate(X, Y):\n",
    "    M = []\n",
    "    for mf, (zscore, distance) in itertools.product(mfs, distances_):\n",
    "        print(mf, zscore, distance.__name__)\n",
    "        rl_token = compute_links([X, 0, mf, zscore, 1e-1, distance])\n",
    "        Mi = evaluate_linking(rl_token, Y)\n",
    "        M.append(Mi)\n",
    "\n",
    "    M = np.array(M).reshape(-1, len(distances_), 3)\n",
    "    return M\n",
    "\n",
    "M_tokens = evaluate(X_token_st_jean, Y_st_jean)\n",
    "M_lemmas = evaluate(X_lemma_st_jean, Y_st_jean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(M, f):\n",
    "    custom_lines = [Line2D([0], [0], color=f\"C{i}\", lw=2) for i in range(len(distances_))]\n",
    "    labels = [d.__name__ for z, d in distances_]\n",
    "\n",
    "    plt.figure(figsize=(6, 8), dpi=200)\n",
    "    for i in range(len(distances_)):\n",
    "        plt.plot(mfs, M[:, i, 0], c=f\"C{i}\")\n",
    "    plt.xlabel(\"$n$-MF\")\n",
    "    plt.ylabel(\"Average Precision (AP)\")\n",
    "    plt.legend(custom_lines, labels, loc=\"lower center\", ncol=2)\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.savefig(f\"img/mf_{f}.png\")\n",
    "    \n",
    "plot(M_tokens, \"tokens\")\n",
    "plot(M_lemmas, \"lemmas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table(M):\n",
    "    d_names = [d[-1].__name__ for d in distances_]\n",
    "    a = M[:,:,0].T\n",
    "    print(\"\\n\".join(d_names))\n",
    "    print(\"\\n\".join(mfs.astype(str)))\n",
    "    print(a)\n",
    "        \n",
    "table(M_tokens)\n",
    "table(M_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance of the text size in stylometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = []\n",
    "\n",
    "sizes = np.arange(9000, 0, -250, dtype=int)\n",
    "\n",
    "for i in sizes:\n",
    "    # limitate the data size\n",
    "    Xi = [x[:i] for x in X_token_st_jean]\n",
    "    rl = compute_links([Xi, 0, 750, True, 0.1, distances.cosine_distance])\n",
    "    m = evaluate_linking(rl, Y_st_jean)\n",
    "    print(i, m)\n",
    "    M.append(m)\n",
    "\n",
    "M = np.array(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sizes)\n",
    "print(M)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(6, 4), dpi=200)\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(sizes, M[:, 0], c=\"C0\", ls=\"solid\",\n",
    "         label=\"Average Precision (AP)\")\n",
    "ax1.plot(sizes, M[:, 1], c=\"C0\", ls=\"dashed\", label=\"RPrec\")\n",
    "ax2.plot(sizes, M[:, 2], c=\"C0\", ls=\"dotted\", label=\"HPRec\")\n",
    "ax1.set_xlabel(\"#Tokens per texts\")\n",
    "plt.gca().invert_xaxis()\n",
    "ax1.set_ylabel(\"AP/RPrec\")\n",
    "ax2.set_ylabel(\"HPrec\")\n",
    "plt.xticks(np.arange(9000, -1, -1000, dtype=int))\n",
    "fig.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/degradation.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequent errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rls = rls_st_jean[0:4]\n",
    "Y = Y_st_jean\n",
    "X = X_token_st_jean\n",
    "\n",
    "top_n = 20\n",
    "keep = 2\n",
    "\n",
    "incorrectly_ranked = defaultdict(lambda: 0)\n",
    "\n",
    "for rl in rls:\n",
    "    m = evaluate_linking(rl, Y)\n",
    "    print(m)\n",
    "    i = 0\n",
    "    for (a, b), s in rl:\n",
    "        if Y[a] != Y[b]:\n",
    "            i += 1\n",
    "            incorrectly_ranked[(a, b)] += 1\n",
    "            if i > top_n:\n",
    "                break\n",
    "\n",
    "top_errors = Counter(dict(incorrectly_ranked)).most_common(keep)\n",
    "print(top_errors)\n",
    "\n",
    "features, mfw = most_frequent_word(X, 750, lidstone_lambda=1e0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(features, a, b, filename):\n",
    "    A, B = features[a, :], features[b, :]\n",
    "    mean = np.mean(np.array([A, B]), axis=0)\n",
    "    order_indices = np.argsort(mean)[::-1]\n",
    "    A = A[order_indices]\n",
    "    B = B[order_indices]\n",
    "    plt.figure(figsize=(4, 3), dpi=200)\n",
    "    plt.yscale(\"log\")\n",
    "    x = range(len(A))\n",
    "    plt.bar(x, A, width=1, label=f\"{Y[a]} ({a+1})\", alpha=0.5)\n",
    "    plt.bar(x, B, width=1, label=f\"{Y[b]} ({b+1})\", alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.xticks([], [])\n",
    "    plt.xlabel(\"750-MF tokens vector\")\n",
    "    plt.ylabel(\"Relative word frequency\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "\n",
    "for i, ((a, b), errors) in enumerate(top_errors):\n",
    "    plot(features, a, b, f\"img/mf_vector_error_{i}.png\")\n",
    "\n",
    "rl = rls[0]\n",
    "(a, b), score = rl[0]\n",
    "plot(features, a, b, f\"img/mf_vector_first_rl.png\")\n",
    "\n",
    "(a, b), score = rl[int(m[-1] - 1)]\n",
    "plot(features, a, b, f\"img/mf_vector_first_last_rl.png\")\n",
    "\n",
    "(a, b), score = rl[-1]\n",
    "plot(features, a, b, f\"img/mf_vector_last_rl.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MF Letters/In-word $n$-grams\n",
    "\n",
    "### Letters $n$-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y = X_token_oxquarry, Y_oxquarry\n",
    "X, Y = X_token_brunet, Y_brunet\n",
    "# X, Y = X_token_st_jean, Y_st_jean\n",
    "\n",
    "M = defaultdict(list)\n",
    "\n",
    "mfws = np.arange(500, 15000 + 1, 500)\n",
    "\n",
    "ngrams_types = [3, 4, 5, (2, 3), (3, 4), (4, 5)]\n",
    "for ngrams_type in ngrams_types:\n",
    "    print(ngrams_type)\n",
    "    for mfw in mfws:\n",
    "        rep = [X, ngrams_type, mfw, True, 1e-1, distances.cosine_distance]\n",
    "        rl = compute_links(rep)\n",
    "        m = evaluate_linking(rl, Y)\n",
    "        M[ngrams_type].append(m)\n",
    "        print(mfw, m)\n",
    "\n",
    "M = dict(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4), dpi=200)\n",
    "for ngrams_type in ngrams_types:\n",
    "    X = mfws\n",
    "    Y = [i[0] for i in M[ngrams_type]]\n",
    "    plt.plot(X, Y, label=f\"Letters {str(ngrams_type)}-grams\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$n$-MF letters $n$-grams\")\n",
    "plt.ylabel(\"Average Precision (AP)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/letter_ngrams.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y = X_token_oxquarry, Y_oxquarry\n",
    "# X, Y = X_token_brunet, Y_brunet\n",
    "X, Y = X_token_st_jean, Y_st_jean\n",
    "\n",
    "configurations = [\n",
    "    (3, 3000),\n",
    "    (4, 8000),\n",
    "]\n",
    "\n",
    "for n_grams_type, mfw in configurations:\n",
    "    for zscore, distance in distances.vector_distances:\n",
    "        rep = [X, n_grams_type, mfw, zscore, 1e-1, distance]\n",
    "        rl = compute_links(rep)\n",
    "        m = evaluate_linking(rl, Y)\n",
    "        print(n_grams_type, mfw, distance.__name__, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-word $n$-grams, $n$-First letters, $n$-Last letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y = X_token_oxquarry, Y_oxquarry\n",
    "# X, Y = X_token_brunet, Y_brunet\n",
    "X, Y = X_token_st_jean, Y_st_jean\n",
    "\n",
    "plt.figure(figsize=(6, 4), dpi=200)\n",
    "\n",
    "for n, c in zip([3, 4, 5], [\"C0\", \"C1\", \"C2\"]):\n",
    "    print(n)\n",
    "    word_begin_X = first_letters_cut(X, n)\n",
    "    word_ngrams_X = word_n_grams(X, n)\n",
    "    word_end_X = last_letters_cut(X, n)\n",
    "\n",
    "    M_ngrams = []\n",
    "    M_first = []\n",
    "    M_last = []\n",
    "\n",
    "    mfs = np.arange(200, 4000 + 1, 100)\n",
    "\n",
    "    for mf in mfs:\n",
    "        print(mf)\n",
    "        rep = [word_ngrams_X, 0, mf, True, 1e-1, distances.cosine_distance]\n",
    "        rl = compute_links(rep)\n",
    "        m = evaluate_linking(rl, Y)\n",
    "        M_ngrams.append(m[0])\n",
    "        rep = [word_begin_X, 0, mf, True, 1e-1, distances.cosine_distance]\n",
    "        rl = compute_links(rep)\n",
    "        m = evaluate_linking(rl, Y)\n",
    "        M_first.append(m[0])\n",
    "        rep = [word_end_X, 0, mf, True, 1e-1, distances.cosine_distance]\n",
    "        rl = compute_links(rep)\n",
    "        m = evaluate_linking(rl, Y)\n",
    "        M_last.append(m[0])\n",
    "\n",
    "    plt.plot(mfs, M_ngrams, c=c, ls=\"solid\")\n",
    "    plt.plot(mfs, M_first, c=c, ls=\"dotted\")\n",
    "    plt.plot(mfs, M_last, c=c, ls=\"dashed\")\n",
    "\n",
    "custom_lines = [\n",
    "    Line2D([0], [0], color=\"C0\", lw=2),\n",
    "    Line2D([0], [0], color=\"C1\", lw=2),\n",
    "    Line2D([0], [0], color=\"C2\", lw=2),\n",
    "    Line2D([0], [0], color=\"k\", lw=2, ls=\"solid\"),\n",
    "    Line2D([0], [0], color=\"k\", lw=2, ls=\"dotted\"),\n",
    "    Line2D([0], [0], color=\"k\", lw=2, ls=\"dashed\"),\n",
    "]\n",
    "\n",
    "plt.legend(custom_lines, [\"n = 3\", \"n = 4\", \"n = 5\", \"In-word $n$-grams\",\n",
    "                          \"$n$-First\", \"$n$-Last\"], loc=\"lower right\")\n",
    "plt.xlabel(\"$n$-MF\")\n",
    "plt.ylabel(\"Average Precision (AP)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/first_last_letters_ngrams.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MF POS $n$-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = X_pos_st_jean, Y_st_jean\n",
    "\n",
    "M = defaultdict(list)\n",
    "\n",
    "mfws = np.arange(100, 2000 + 1, 100)\n",
    "\n",
    "ngrams_types = [2, 3, 4, (2, 3)]\n",
    "for ngrams_type in ngrams_types:\n",
    "    print(ngrams_type)\n",
    "    for mf in mfs:\n",
    "        rep = [X, ngrams_type, mf, True, 1e-1, distances.cosine_distance]\n",
    "        rl = compute_links(rep)\n",
    "        m = evaluate_linking(rl, Y)\n",
    "        M[ngrams_type].append(m)\n",
    "        print(mf, m)\n",
    "\n",
    "M = dict(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4), dpi=200)\n",
    "for ngrams_type in ngrams_types:\n",
    "    X = mfs\n",
    "    Y = [i[0] for i in M[ngrams_type]]\n",
    "    plt.plot(X, Y, label=f\"{str(ngrams_type)}-POS\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$n$-MF\")\n",
    "plt.ylabel(\"Average Precision (AP)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/n_pos.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = X_pos_st_jean, Y_st_jean\n",
    "\n",
    "M = defaultdict(list)\n",
    "\n",
    "configurations = [\n",
    "    (2, 250),\n",
    "    (3, 1000),\n",
    "]\n",
    "\n",
    "for ngrams_type, mf in configurations:\n",
    "    for zscore, distance in distances.vector_distances:\n",
    "        rep = [X, ngrams_type, mf, zscore, 1e-1, distance]\n",
    "        rl = compute_links(rep)\n",
    "        print(ngrams_type, mf, distance.__name__, evaluate_linking(rl, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Every tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    distances.vector_distances,\n",
    "    [\n",
    "        (X_token_oxquarry, Y_oxquarry),\n",
    "        (X_token_brunet, Y_brunet),\n",
    "        (X_token_st_jean, Y_st_jean),\n",
    "    ],\n",
    "]\n",
    "\n",
    "def do(mfw, remove_hapax):\n",
    "    M = []\n",
    "    for param in tqdm(list(itertools.product(*params))):\n",
    "        (zscore, dist), (X, Y) = param\n",
    "        rl = compute_links([X, 0, mfw, zscore, 1e-1, dist, remove_hapax])\n",
    "        m = evaluate_linking(rl, Y)\n",
    "        print(m, zscore, dist)\n",
    "        M.append(m[0])\n",
    "\n",
    "    M = np.array(M)\n",
    "    M = M.reshape([len(p) for p in params])\n",
    "    return M\n",
    "\n",
    "M_750 = do(750, True)\n",
    "M_without_hapax = do(np.inf, True)\n",
    "M_with_hapax = do(np.inf, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_M(M):\n",
    "    for l, m in zip(M, M.mean(axis=1)):\n",
    "        print(l, f\"{m:.2f}\")\n",
    "    print(f\"-- {M.mean(axis=0)} {M.mean():.2f}\")\n",
    "    print()\n",
    "    \n",
    "for d in distances.vector_distances:\n",
    "    print(d[-1].__name__.capitalize())\n",
    "\n",
    "print_M(M_with_hapax)\n",
    "print_M(M_without_hapax - M_with_hapax)\n",
    "print_M(M_750 - M_with_hapax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compression based distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y = X_token_oxquarry, Y_oxquarry\n",
    "X, Y = X_token_brunet, Y_brunet\n",
    "# X, Y = X_token_st_jean, Y_st_jean\n",
    "\n",
    "compression_methods = [\n",
    "    compressions.lzma,\n",
    "    compressions.bz2,\n",
    "    compressions.gzip,\n",
    "]\n",
    "distance_funcs = [\n",
    "    distances.ncd,\n",
    "    distances.cbc,\n",
    "]\n",
    "distances_compressions = list(itertools.product(\n",
    "    compression_methods, distance_funcs))\n",
    "\n",
    "M = []\n",
    "T = []\n",
    "\n",
    "for i in range(3):\n",
    "    for compression_method, distance_func in distances_compressions:\n",
    "        print(compression_method.__name__, distance_func.__name__)\n",
    "        t0 = time.time()\n",
    "        rep = (X, compression_method, distance_func)\n",
    "        rl = compute_links(rep)\n",
    "        t = time.time() - t0\n",
    "        m = evaluate_linking(rl, Y)\n",
    "        M.append(m)\n",
    "        T.append(t)\n",
    "        print(m, t)\n",
    "\n",
    "M = np.array(M).reshape(-1, len(distances_compressions), 3)\n",
    "T = np.array(T).reshape(-1, len(distances_compressions))\n",
    "M = M.mean(axis=0)\n",
    "T = T.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M)\n",
    "print(T)\n",
    "\n",
    "plt.figure(figsize=(6, 4), dpi=200)\n",
    "x, y, c = M[:, 1], M[:, 0], M[:, 2]\n",
    "plt.scatter(x, y, c=c, marker=\".\")\n",
    "texts = []\n",
    "for i, (compression_method, distance_func) in enumerate(distances_compressions):\n",
    "    text = f\"({compression_method.__name__}, {distance_func.__name__})\"\n",
    "    xy = (x[i], y[i])\n",
    "    texts.append(plt.annotate(text, xy))\n",
    "adjust_text(texts)\n",
    "cbar = plt.colorbar()\n",
    "plt.xlabel(\"RPrec\")\n",
    "plt.ylabel(\"Average precision (AP)\")\n",
    "cbar.set_label(\"HPrec\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/compression_evaluation.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual methods summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retained text representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr9(X_token, X_pos):\n",
    "    return [\n",
    "        [X_token, 0, 750, True, 1e-1, distances.cosine_distance],\n",
    "        [X_token, 0, 750, False, 1e-1, distances.clark],\n",
    "        [X_token, 0, 750, True, 1e-1, distances.manhattan],\n",
    "        [X_token, 0, 750, False, 1e-1, distances.tanimoto],\n",
    "        [X_token, 3, 3000, True, 1e-1, distances.cosine_distance],\n",
    "        [X_token, 4, 8000, True, 1e-1, distances.cosine_distance],\n",
    "        (X_token, compressions.bz2, distances.cbc),\n",
    "        [X_pos, 2, 250, True, 1e-1, distances.cosine_distance],\n",
    "        [X_pos, 3, 1000, True, 1e-1, distances.manhattan],\n",
    "    ]\n",
    "\n",
    "\n",
    "def tr7(X_token):\n",
    "    return [\n",
    "        [X_token, 0, 750, True, 1e-1, distances.cosine_distance],\n",
    "        [X_token, 0, 750, False, 1e-1, distances.clark],\n",
    "        [X_token, 0, 750, True, 1e-1, distances.manhattan],\n",
    "        [X_token, 0, 750, False, 1e-1, distances.tanimoto],\n",
    "        [X_token, 3, 3000, True, 1e-1, distances.cosine_distance],\n",
    "        [X_token, 4, 8000, True, 1e-1, distances.cosine_distance],\n",
    "        (X_token, compressions.bz2, distances.cbc),\n",
    "    ]\n",
    "\n",
    "\n",
    "def tr(*X):\n",
    "    if len(X) == 2:\n",
    "        return tr9(X[0], X[1])\n",
    "    else:\n",
    "        return tr7(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rls_oxquarry = [compute_links(t) for t in tqdm(tr(X_token_oxquarry))]\n",
    "print(np.array([evaluate_linking(rl, Y_oxquarry) for rl in rls_oxquarry]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rls_brunet = [compute_links(t) for t in tqdm(tr(X_token_brunet))]\n",
    "print(np.array([evaluate_linking(rl, Y_brunet) for rl in rls_brunet]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rls_st_jean_A = [compute_links(t) for t in tqdm(tr(X_token_st_jean_A, X_pos_st_jean_A))]\n",
    "print(np.array([evaluate_linking(rl, Y_st_jean_A) for rl in rls_st_jean_A]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rls_st_jean_B = [compute_links(t) for t in tqdm(tr(X_token_st_jean_B, X_pos_st_jean_B))]\n",
    "print(np.array([evaluate_linking(rl, Y_st_jean_B) for rl in rls_st_jean_B]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_rls = [\n",
    "    (\"Oxquarry\", rls_oxquarry, Y_oxquarry),\n",
    "    (\"Brunet\", rls_brunet, Y_brunet),\n",
    "    (\"St-Jean A\", rls_st_jean_A, Y_st_jean_A),\n",
    "    (\"St-Jean B\", rls_st_jean_B, Y_st_jean_B),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rls_st_jean = [compute_links(t) for t in tqdm(tr(X_token_st_jean, X_pos_st_jean))]\n",
    "print(np.array([evaluate_linking(rl, Y_st_jean) for rl in rls_st_jean]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publication date differences analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl = rl_z_score_st_jean\n",
    "Y = Y_st_jean\n",
    "info = info_st_jean\n",
    "\n",
    "Y = np.array(Y)\n",
    "s = 5\n",
    "\n",
    "dates = [int(i[-1]) for i in info]\n",
    "plt.figure(figsize=(4, 3), dpi=200)\n",
    "plt.hist(dates, bins=np.arange(np.min(dates), np.max(dates), s), density=True, alpha=0.7)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/dates_distribution.png\")\n",
    "\n",
    "print(evaluate_linking(rl, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = np.array([link for link, s in rl])\n",
    "\n",
    "date_diffs = np.array([np.abs(dates[a] - dates[b]) for (a, b) in links])\n",
    "\n",
    "true_links = np.array([Y[a] == Y[b] for (a, b) in links])\n",
    "\n",
    "r = compute_r(Y)\n",
    "all_links_date_diff = date_diffs\n",
    "true_links_date_diff = date_diffs[true_links]\n",
    "false_links_date_diff = date_diffs[~true_links]\n",
    "\n",
    "top_r_true_links_date_diff = true_links_date_diff[0:r]\n",
    "top_r_false_links_date_diff = false_links_date_diff[0:r]\n",
    "\n",
    "print(\"all_links\", all_links_date_diff.mean(), all_links_date_diff.std())\n",
    "print(\"true_links\", true_links_date_diff.mean(), true_links_date_diff.std())\n",
    "print(\"false_links\", false_links_date_diff.mean(), false_links_date_diff.std())\n",
    "\n",
    "\n",
    "print(\"largest true link date diff\", true_links_date_diff.max(), links[true_links][true_links_date_diff.argmax()]+1)\n",
    "print(\"true links = 0 : \", np.sum(true_links_date_diff == 0), \"true links >= 5 : \", np.sum(true_links_date_diff <= 5), \"true links total : \", np.sum(true_links))\n",
    "\n",
    "print(\"Common date diffs for top-r false links > 35\", Counter(top_r_false_links_date_diff[top_r_false_links_date_diff > 35]).most_common())\n",
    "\n",
    "common_false_link = links[~true_links][0:r][top_r_false_links_date_diff == 62]\n",
    "for a, b in common_false_link:\n",
    "    print(a+1, Y[a], b+1, Y[b])\n",
    "\n",
    "bins = np.arange(0, np.max(all_links_date_diff), s)\n",
    "ticks = np.arange(date_diffs.min(), date_diffs.max(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [true_links_date_diff, false_links_date_diff]\n",
    "colors = [\"C2\", \"C3\"]\n",
    "\n",
    "plt.figure(figsize=(4, 3), dpi=200)\n",
    "\n",
    "params = {\n",
    "    \"bins\" : bins,\n",
    "    \"color\" : colors,\n",
    "    \"alpha\" : 0.7,\n",
    "    \"density\" : True,\n",
    "    \"stacked\": True,\n",
    "}\n",
    "n, bins, patches = plt.hist(data, **params)\n",
    "for d, c in zip(data, colors):\n",
    "    mean, std = d.mean(), d.std()\n",
    "    params = {\n",
    "        \"c\" : c,\n",
    "        \"linestyle\" : \"dashed\",\n",
    "    }\n",
    "    plt.axvline(mean, **params)\n",
    "    params = {\n",
    "        \"y\" : (n.max() - n.min()) / 2,\n",
    "        \"xmin\" : mean - std // 2,\n",
    "        \"xmax\" : mean + std // 2,\n",
    "        \"color\" : c,\n",
    "        \"linestyle\" : \"solid\",\n",
    "    }\n",
    "    plt.hlines(**params)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Date difference\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xticks(ticks)\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color=\"k\", lw=1, ls=\"dashed\", label=\"Mean\"),\n",
    "    Line2D([0], [0], color=\"k\", lw=1, ls=\"solid\", label=\"Std\"),\n",
    "    Line2D([0], [0], color=\"C2\", alpha=0.7, lw=4, label=\"True links\"),\n",
    "    Line2D([0], [0], color=\"C3\", alpha=0.7, lw=4, label=\"False links\"),\n",
    "]\n",
    "\n",
    "plt.legend(handles=legend_elements)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/dates_differences_true_false.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3), dpi=200)\n",
    "data = top_r_false_links_date_diff\n",
    "color = \"C1\"\n",
    "\n",
    "params = {\n",
    "    \"bins\" : bins,\n",
    "    \"color\" : color,\n",
    "    \"alpha\" : 0.7,\n",
    "    \"density\" : True,\n",
    "}\n",
    "n, bins, patches = plt.hist(data, **params)\n",
    "mean, std = data.mean(), data.std()\n",
    "params = {\n",
    "    \"c\" : color,\n",
    "    \"linestyle\" : \"dashed\",\n",
    "}\n",
    "plt.axvline(mean, **params)\n",
    "params = {\n",
    "    \"y\" : (n.max() - n.min()) / 2,\n",
    "    \"xmin\" : mean - std // 2,\n",
    "    \"xmax\" : mean + std // 2,\n",
    "    \"color\" : color,\n",
    "    \"linestyle\" : \"solid\",\n",
    "}\n",
    "plt.hlines(**params)\n",
    "\n",
    "plt.xlabel(\"Date difference\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xticks(ticks)\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color=\"k\", lw=1, ls=\"dashed\", label=\"Mean\"),\n",
    "    Line2D([0], [0], color=\"k\", lw=1, ls=\"solid\", label=\"Std\"),\n",
    "    Line2D([0], [0], color=\"C1\", alpha=0.7, lw=4, label=\"top-r False links\"),\n",
    "]\n",
    "\n",
    "plt.legend(handles=legend_elements)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/dates_differences_r_false.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distances matrix visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl = rls_oxquarry[1]\n",
    "Y = Y_oxquarry\n",
    "# rl = rls_oxquarry[3]\n",
    "# Y = Y_oxquarry\n",
    "\n",
    "distances_matrix = distances_matrix_from_rank_list(rl)\n",
    "\n",
    "# subset = [\"Sand\", \"Stael\", \"Gautier\", \"Regnier\"]\n",
    "# Y, distances_matrix = subset_Y_and_distance_matrix(Y, distances_matrix, subset)\n",
    "Y, distances_matrix = sort_Y_and_distance_matrix(Y, distances_matrix)\n",
    "\n",
    "# distances_matrix = np.zeros(distances_matrix.shape)\n",
    "\n",
    "ticks = []\n",
    "prev = None\n",
    "for i, label in enumerate(Y):\n",
    "    if prev != label:\n",
    "        ticks.append(i)\n",
    "    prev = label\n",
    "labels = np.unique(Y)\n",
    "    \n",
    "plt.figure(figsize=(4,3), dpi=200)\n",
    "plt.imshow(distances_matrix, cmap=\"Blues\")\n",
    "plt.xticks(ticks, labels, rotation=\"vertical\", fontsize=\"xx-small\")\n",
    "plt.yticks(ticks, labels, rotation=\"horizontal\", fontsize=\"xx-small\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/distance_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.normal(loc=1, scale=0.5, size=(100, 2))\n",
    "B = np.random.normal(loc=-1, scale=0.5, size=(100, 2))\n",
    "\n",
    "AB = np.array(list(A) + list(B))\n",
    "\n",
    "plt.figure(figsize=(4,3), dpi=200)\n",
    "plt.scatter(AB[:,0], AB[:,1], c=\"k\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/clustering_example_1.png\")\n",
    "\n",
    "plt.figure(figsize=(4,3), dpi=200)\n",
    "plt.scatter(A[:,0], A[:,1])\n",
    "plt.scatter(B[:,0], B[:,1])\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/clustering_example_2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierachical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mc = []\n",
    "Ml = []\n",
    "\n",
    "for name, rls, Y in datasets_rls:\n",
    "    for (i, rl) in enumerate(rls):\n",
    "        f1_a = best_clustering(rl, \"single\", Y)\n",
    "        f1_b = best_clustering(rl, \"average\", Y)\n",
    "        f1_c = best_clustering(rl, \"complete\", Y)\n",
    "        Mc.append([f1_a, f1_b, f1_c])\n",
    "        Ml.append(evaluate_linking(rl, Y))\n",
    "        \n",
    "        print(f\"{i} & {name} & {f1_a:.2f} & {f1_b:.2f} & {f1_c:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mc = np.array(Mc)\n",
    "Ml = np.array(Ml)\n",
    "\n",
    "i = 0\n",
    "for name, rls, Y in datasets_rls:\n",
    "    l = len(rls)\n",
    "    print(np.mean(Mc[i:i+l, :], axis=0))\n",
    "    i += l\n",
    "\n",
    "print(np.mean(Mc, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation AP/B3F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_linear_regression(X, Y):\n",
    "    reg = linregress(X, Y)\n",
    "    slope, intercept, r_value, p_value, std_err = reg\n",
    "    print(slope, intercept)\n",
    "    print(f\"{r_value:.2f} & {p_value:.2e} & {std_err:.2e}\")\n",
    "    def f(x):\n",
    "        return slope * x + intercept\n",
    "    Xs = np.array([np.min(X), np.max(X)])\n",
    "    Ys = f(Xs)\n",
    "    plt.plot(Xs, Ys, label=\"Linear regression\")\n",
    "\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(5,4), dpi=200)\n",
    "    plt.scatter(Ml[:, 0], Mc[:, i], label=\"Rank list / resulting clustering\")\n",
    "    plot_linear_regression(Ml[:, 0], Mc[:, i])\n",
    "    plt.xlabel(\"AP\")\n",
    "    plt.ylabel(\"$B^3_{F_1}$\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"img/correlation_average_precision_b3f1_{i}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette-based Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(rl, Y, linkage, f):\n",
    "    labels, silhouette_scores = silhouette_based_clustering(rl, linkage=linkage)\n",
    "    ns, labels_list = clustering_at_every_n_clusters(rl, linkage=linkage)\n",
    "    evaluations = np.array([evaluate_clustering(Y, labels) for labels in labels_list])\n",
    "\n",
    "    n_clusters_found = len(np.unique(labels))\n",
    "    n_clusters_actual = len(np.unique(Y))\n",
    "\n",
    "    plt.figure(figsize=(6, 4), dpi=200)\n",
    "    plt.plot(ns, evaluations[:, 1], label=\"$B^3_{precision}$\")\n",
    "    plt.plot(ns, evaluations[:, 2], label=\"$B^3_{recall}$\")\n",
    "    plt.plot(ns, evaluations[:, 0], label=\"$B^3_{F_1}$\")\n",
    "    plt.plot(ns, evaluations[:, 3], label=\"$r_{diff}$\")\n",
    "    plt.plot(*silhouette_scores, label=\"Mean Silhouette\")\n",
    "    plt.axvline(n_clusters_found, 0, 1, ls=\"dashed\", c=\"C4\", label=\"Maximal Mean Silhouette\")\n",
    "    plt.axvline(n_clusters_actual, 0, 1, ls=\"dashed\", c=\"C2\", label=\"Actual #Clusters\")\n",
    "    xmin, xmax, ymin, ymax = plt.axis()\n",
    "    ypos = ymax / 2 - ymin / 2\n",
    "    plt.text(n_clusters_found, ypos, f\"{n_clusters_found}\", c=\"C4\", rotation=\"vertical\")\n",
    "    plt.text(n_clusters_actual, ypos, f\"{n_clusters_actual}\", c=\"C2\", rotation=\"vertical\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(\"#Clusters\")\n",
    "    plt.ylabel(\"Metric\")\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f)\n",
    "    \n",
    "plot(rl_z_score_st_jean_A, Y_st_jean_A, \"average\", \"img/silhouette_based_clustering_st_jean_A_average.png\")\n",
    "plot(rl_z_score_st_jean_B, Y_st_jean_B, \"average\", \"img/silhouette_based_clustering_st_jean_B_average.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mc = []\n",
    "# alpha = 0\n",
    "alpha = -0.2\n",
    "\n",
    "for name, rls, Y in datasets_rls:\n",
    "    for (i, rl) in enumerate(rls):\n",
    "        labels_a, _ = silhouette_based_clustering(rl, \"single\", alpha)\n",
    "        m_a = evaluate_clustering(Y, labels_a)\n",
    "        \n",
    "        labels_b, _ = silhouette_based_clustering(rl, \"average\", alpha)\n",
    "        m_b = evaluate_clustering(Y, labels_b)\n",
    "        \n",
    "        labels_c, _ = silhouette_based_clustering(rl, \"complete\", alpha)\n",
    "        m_c = evaluate_clustering(Y, labels_c)\n",
    "        \n",
    "        Mc.append([m_a, m_b, m_c])\n",
    "        \n",
    "        print(f\"{i} & {name} & {m_a[[0,-1]]} & {m_b[[0,-1]]} & {m_c[[0,-1]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mc = np.abs(np.array(Mc))\n",
    "Ml = np.abs(np.array(Ml))\n",
    "\n",
    "i = 0\n",
    "for name, rls, Y in datasets_rls:\n",
    "    l = len(rls)\n",
    "    print(np.mean(Mc[i:i+l, :], axis=0)[:,[0,-1]])\n",
    "    i += l\n",
    "\n",
    "print(np.mean(Mc, axis=0)[:,[0,-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution-based Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution-based model example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(rl, Y, xlabel, f):\n",
    "    print(evaluate_linking(rl, Y))\n",
    "    links = np.array(list(zip(*rl))[0])\n",
    "    scores = np.array(list(zip(*rl))[1])\n",
    "    labels = np.array([Y[a] == Y[b] for a, b in links])\n",
    "    \n",
    "    # if it's not a probability normalize between 0 and 1\n",
    "    if np.max(scores) > 1 or np.min(scores) < 0:\n",
    "        scores = normalize(scores, 0, 1)\n",
    "\n",
    "    plt.figure(figsize=(4,3), dpi=200)\n",
    "    bins = 20\n",
    "    plt.hist(scores[labels], bins=bins, density=True, label=\"True links\", alpha=0.5)\n",
    "    plt.hist(scores[~labels], bins=bins, density=True, label=\"False links\", alpha=0.5)\n",
    "\n",
    "    beta_true = fit_beta(scores[labels])\n",
    "    x_true = np.linspace(np.min(scores[labels])+1e-2, np.max(scores[labels])-1e-2, 200)\n",
    "    y_true = beta_true.pdf(x_true)\n",
    "    \n",
    "    beta_false = fit_beta(scores[~labels])\n",
    "    x_false = np.linspace(np.min(scores[~labels])+1e-2, np.max(scores[~labels])-1e-2, 200)\n",
    "    y_false = beta_false.pdf(x_false)\n",
    " \n",
    "    plt.plot(x_true, y_true, c=\"C0\")\n",
    "    plt.plot(x_false, y_false, c=\"C1\")\n",
    "    \n",
    "    a = find_two_beta_same_area(beta_true, beta_false)\n",
    "\n",
    "    plt.axvline(a, color=\"k\", ls=\"dashed\", label=\"Equiprobable\")\n",
    "    \n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f)\n",
    "\n",
    "rl = rls_st_jean_B[0]\n",
    "Y = Y_st_jean_B\n",
    "\n",
    "plot(rl, Y, \"Normalized distance\", \"img/links_score_density.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mc = []\n",
    "\n",
    "dts = []\n",
    "\n",
    "for name, rls, Y in datasets_rls:\n",
    "    dts.append([dist_thresh_two_beta(rl, Y) for rl in rls])\n",
    "\n",
    "for (training_id, dts_), (name, rls, Y) in itertools.product(enumerate(dts), datasets_rls):\n",
    "    \n",
    "    Mi = []\n",
    "    \n",
    "    for (i, rl), dt in zip(enumerate(rls), dts_):\n",
    "        labels_a = clustering_at_dist_thresh(rl, \"single\", dt)\n",
    "        m_a = evaluate_clustering(Y, labels_a)\n",
    "        \n",
    "        labels_b = clustering_at_dist_thresh(rl, \"average\", dt)\n",
    "        m_b = evaluate_clustering(Y, labels_b)\n",
    "        \n",
    "        labels_c = clustering_at_dist_thresh(rl, \"complete\", dt)\n",
    "        m_c = evaluate_clustering(Y, labels_c)\n",
    "        \n",
    "        Mi.append([m_a, m_b, m_c])\n",
    "        \n",
    "        print(f\"{i} & {name} & {m_a[[0,-1]]} & {m_b[[0,-1]]} & {m_c[[0,-1]]}\")\n",
    "        \n",
    "    Mi = np.array(Mi)\n",
    "    Mc.append(np.mean(np.abs(Mi), axis=(0)))\n",
    "        \n",
    "Mc = np.array(Mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mc = Mc.reshape((-1, len(datasets_rls), 3, 4))\n",
    "print(Mc.shape)\n",
    "\n",
    "print(\"Single\")\n",
    "print(Mc[:,:,0,[0, -1]])\n",
    "print(np.mean(Mc[:,:,0,[0, -1]],axis=(0)))\n",
    "print(np.mean(Mc[:,:,0,[0, -1]],axis=(1)))\n",
    "print(np.mean(Mc[:,:,0,[0, -1]],axis=(0,1)))\n",
    "\n",
    "print(\"Average\")\n",
    "print(Mc[:,:,1,[0, -1]])\n",
    "print(np.mean(Mc[:,:,1,[0, -1]],axis=(0)))\n",
    "print(np.mean(Mc[:,:,1,[0, -1]],axis=(1)))\n",
    "print(np.mean(Mc[:,:,1,[0, -1]],axis=(0,1)))\n",
    "\n",
    "print(\"Complete\")\n",
    "print(Mc[:,:,2,[0, -1]])\n",
    "print(np.mean(Mc[:,:,2,[0, -1]],axis=(0)))\n",
    "print(np.mean(Mc[:,:,2,[0, -1]],axis=(1)))\n",
    "print(np.mean(Mc[:,:,2,[0, -1]],axis=(0,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression-based clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression-based model example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl = rl_z_score_st_jean_B\n",
    "Y = Y_st_jean_B\n",
    "\n",
    "X_rl = features_from_rank_list(rl)\n",
    "Y_rl = labels_from_rank_list(rl, Y)\n",
    "\n",
    "model = LogisticRegression(random_state=0).fit(X_rl, Y_rl)\n",
    "\n",
    "X_rl = np.array(X_rl)\n",
    "Y_rl = np.array(Y_rl)\n",
    "\n",
    "X_true = X_rl[Y_rl == 1]\n",
    "X_false = X_rl[Y_rl == 0]\n",
    "\n",
    "min_ = X_rl.min(axis=0)\n",
    "max_ = X_rl.max(axis=0)\n",
    "\n",
    "print(min_, max_)\n",
    "\n",
    "n = 250\n",
    "x = np.linspace(min_[0], max_[0], n)\n",
    "y = np.linspace(min_[1], max_[1], n)\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "\n",
    "X_grid = np.array([xv, yv])\n",
    "X_grid = X_grid.swapaxes(0, 2).reshape((-1, 2))\n",
    "\n",
    "p = model.predict_proba(X_grid)\n",
    "\n",
    "plt.figure(figsize=(6,5), dpi=150)\n",
    "\n",
    "plt.scatter(X_grid[:, 0], X_grid[:, 1], c=1-p[:, 0], marker=\",\", alpha=1, s=1, cmap=\"RdYlGn\")\n",
    "\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(\"True link model probability\")\n",
    "\n",
    "plt.scatter(X_false[:, 0], X_false[:, 1], alpha=0.5, s=6, c=\"C1\", label=\"Actual false links\")\n",
    "plt.scatter(X_true[:, 0], X_true[:, 1], alpha=0.5, s=6, c=\"C0\", label=\"Actual true links\")\n",
    "\n",
    "plt.xlabel(\"$\\log rank/|L|$\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/logistic_example.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mc = []\n",
    "\n",
    "for (name_training, rls_training, Y_training), (name_testing, rls_testing, Y_testing) in itertools.product(datasets_rls, datasets_rls):\n",
    "        \n",
    "    Mi = []\n",
    "    \n",
    "    for i, (rl_training, rl_testing) in enumerate(zip(rls_training, rls_testing)):\n",
    "        dt = dist_thresh_logistic_regression(rl_training, Y_training, rl_testing)\n",
    "        \n",
    "        labels_a = clustering_at_dist_thresh(rl_testing, \"single\", dt)\n",
    "        m_a = evaluate_clustering(Y_testing, labels_a)\n",
    "        \n",
    "        labels_b = clustering_at_dist_thresh(rl_testing, \"average\", dt)\n",
    "        m_b = evaluate_clustering(Y_testing, labels_b)\n",
    "        \n",
    "        labels_c = clustering_at_dist_thresh(rl_testing, \"complete\", dt)\n",
    "        m_c = evaluate_clustering(Y_testing, labels_c)\n",
    "        \n",
    "        Mi.append([m_a, m_b, m_c])\n",
    "        \n",
    "        print(f\"{i} {name_training} & {name_testing} & {m_a[[0,-1]]} & {m_b[[0,-1]]} & {m_c[[0,-1]]}\")\n",
    "        \n",
    "    Mi = np.array(Mi)\n",
    "    Mc.append(np.mean(np.abs(Mi), axis=(0)))\n",
    "        \n",
    "Mc = np.array(Mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mc = Mc.reshape((-1, len(datasets_rls), 3, 4))\n",
    "print(Mc.shape)\n",
    "\n",
    "print(\"Single\")\n",
    "print(Mc[:,:,0,[0, -1]])\n",
    "print(np.mean(Mc[:,:,0,[0, -1]],axis=(0)))\n",
    "print(np.mean(Mc[:,:,0,[0, -1]],axis=(1)))\n",
    "print(np.mean(Mc[:,:,0,[0, -1]],axis=(0,1)))\n",
    "\n",
    "print(\"Average\")\n",
    "print(Mc[:,:,1,[0, -1]])\n",
    "print(np.mean(Mc[:,:,1,[0, -1]],axis=(0)))\n",
    "print(np.mean(Mc[:,:,1,[0, -1]],axis=(1)))\n",
    "print(np.mean(Mc[:,:,1,[0, -1]],axis=(0,1)))\n",
    "\n",
    "print(\"Complete\")\n",
    "print(Mc[:,:,2,[0, -1]])\n",
    "print(np.mean(Mc[:,:,2,[0, -1]],axis=(0)))\n",
    "print(np.mean(Mc[:,:,2,[0, -1]],axis=(1)))\n",
    "print(np.mean(Mc[:,:,2,[0, -1]],axis=(0,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_z_score_oxquarry = fusion_z_score(rls_oxquarry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_z_score_brunet = fusion_z_score(rls_brunet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_z_score_st_jean_A = fusion_z_score(rls_st_jean_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_z_score_st_jean_B = fusion_z_score(rls_st_jean_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_zscores = [\n",
    "    (\"Oxquarry\", rl_z_score_oxquarry, Y_oxquarry),\n",
    "    (\"Brunet\", rl_z_score_brunet, Y_brunet),\n",
    "    (\"St-Jean A\", rl_z_score_st_jean_A, Y_st_jean_A),\n",
    "    (\"St-Jean B\", rl_z_score_st_jean_B, Y_st_jean_B),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_z_score_st_jean = fusion_z_score(rls_st_jean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_zscores = []\n",
    "for name, rl, Y in datasets_zscores:\n",
    "    m = evaluate_linking(rl, Y)\n",
    "    print(name, m)\n",
    "    M_zscores.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_zscores = np.array(M_zscores)\n",
    "print(np.mean(M_zscores, axis=(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rls_regression = []\n",
    "M_regression = []\n",
    "\n",
    "for name_testing, rls_testing, Y_testing in datasets_rls:\n",
    "    rls = []\n",
    "    ms = [] \n",
    "    for name_training, rls_training, Y_training in datasets_rls:\n",
    "        models = [fusion_regression_training(rl, Y_training)[0] for rl in rls_training]\n",
    "        rl = fusion_regression(models, rls_testing)\n",
    "        m = evaluate_linking(rl, Y_testing)\n",
    "        rls.append(rl)\n",
    "        ms.append(m)\n",
    "        print(name_testing, name_training, m)\n",
    "    rls_regression.append(rls)\n",
    "    M_regression.append(ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_regression = np.array(M_regression)\n",
    "\n",
    "print(np.mean(M_regression, axis=(0)))\n",
    "print(np.mean(M_regression, axis=(1)))\n",
    "print(np.mean(M_regression, axis=(0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ml = []\n",
    "\n",
    "for name, rls, Y in datasets_rls:\n",
    "    Mi = []\n",
    "    for rl in rls:\n",
    "        m = evaluate_linking(rl, Y)\n",
    "        Mi.append(m)\n",
    "    Ml.append(np.mean(Mi, axis=0))\n",
    "    \n",
    "Ml = np.array(Ml)\n",
    "print(np.mean(Ml, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank list fusion evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rls_training, Y_training = rls_st_jean_B, Y_st_jean_B\n",
    "rls_testing, Y_testing = rls_st_jean_A, Y_st_jean_A\n",
    "\n",
    "# rls_training, Y_training = rls_st_jean_A, Y_st_jean_A\n",
    "# rls_testing, Y_testing = rls_st_jean_B, Y_st_jean_B\n",
    "\n",
    "fusion_size = 4\n",
    "\n",
    "models = []\n",
    "print(\"Training rank lists\")\n",
    "for i, rl in enumerate(rls_training):\n",
    "    model, rmse = fusion_regression_training(rl, Y_training)\n",
    "    models.append(model)\n",
    "    mesures = evaluate_linking(rl, Y_training)\n",
    "    print(i, mesures, np.array([rmse]))\n",
    "\n",
    "M_single = []\n",
    "rank_lists = []\n",
    "print(\"Testing rank lists\")\n",
    "for i, rl in enumerate(rls_testing):\n",
    "    rank_lists.append(rl)\n",
    "    mesures = evaluate_linking(rl, Y_testing)\n",
    "    M_single.append(mesures)\n",
    "    print(i, mesures)\n",
    "\n",
    "M_single = np.array(M_single)\n",
    "\n",
    "M_single_max = []\n",
    "M_single_mean = []\n",
    "M_fusion_z_score = []\n",
    "M_fusion_regression = []\n",
    "\n",
    "tr_ids = np.array(\n",
    "    list(itertools.combinations(range(len(rls_training)), fusion_size)))\n",
    "\n",
    "for tr_id in tr_ids:\n",
    "    rls = [rank_lists[i] for i in tr_id]\n",
    "\n",
    "    m_single_max = np.max(M_single[tr_id, :], axis=0)\n",
    "    M_single_max.append(m_single_max)\n",
    "\n",
    "    m_single_mean = np.mean(M_single[tr_id, :], axis=0)\n",
    "    M_single_mean.append(m_single_mean)\n",
    "\n",
    "    rl_z_score = fusion_z_score(rls)\n",
    "    m_z_score = evaluate_linking(rl_z_score, Y_testing)\n",
    "    M_fusion_z_score.append(m_z_score)\n",
    "\n",
    "    rl_regression = fusion_regression(models, rls)\n",
    "    m_regression = evaluate_linking(rl_regression, Y_testing)\n",
    "    M_fusion_regression.append(m_regression)\n",
    "\n",
    "M_single_max = np.array(M_single_max)\n",
    "M_single_mean = np.array(M_single_mean)\n",
    "M_fusion_z_score = np.array(M_fusion_z_score)\n",
    "M_fusion_regression = np.array(M_fusion_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plot\")\n",
    "plt.figure(figsize=(6, 4), dpi=200)\n",
    "x, y, c = M_single[:, 1], M_single[:, 0], M_single[:, 2]\n",
    "plt.scatter(x, y, c=c, marker=\"o\", label=\"Single rank list\", alpha=0.8)\n",
    "x, y, c = M_fusion_regression[:,\n",
    "                              1], M_fusion_regression[:, 0], M_fusion_regression[:, 2]\n",
    "plt.scatter(x, y, c=c, marker=\"x\",\n",
    "            label=f\"Regression fusions ({fusion_size} lists)\", alpha=0.5)\n",
    "x, y, c = M_fusion_z_score[:,\n",
    "                           1], M_fusion_z_score[:, 0], M_fusion_z_score[:, 2]\n",
    "plt.scatter(x, y, c=c, marker=\"+\",\n",
    "            label=f\"Z-score fusions ({fusion_size} lists)\", alpha=0.5)\n",
    "cbar = plt.colorbar()\n",
    "plt.xlabel(\"RPrec\")\n",
    "plt.ylabel(\"Average precision (AP)\")\n",
    "cbar.set_label(\"HPrec\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/fusion_evaluation.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fusion Statistics\")\n",
    "\n",
    "def print_statistics_latex(M_list):\n",
    "    values = [f\"${v:.2f}$\" for v in M_list.min(axis=0)]\n",
    "    print(\"Min &\", \" & \".join(values), r\"\\\\\")\n",
    "    values = [f\"${v1:.2f}\\pm{v2:.2f}$\" for v1, v2 in zip(M_list.mean(axis=0), M_list.std(axis=0))]\n",
    "    print(\"Avg$\\pm$Std &\", \" & \".join(values), r\"\\\\\")\n",
    "    values = [f\"${v:.2f}$\" for v in M_list.max(axis=0)]\n",
    "    print(\"Max &\", \" & \".join(values), r\"\\\\\")\n",
    "    argmin = tr_ids[np.argmin(M_list, axis=0)]\n",
    "    print(\"Argmin &\", \" & \".join(\n",
    "        [np.array2string(a, separator=\",\") for a in argmin]), r\"\\\\\")\n",
    "    argmax = tr_ids[np.argmax(M_list, axis=0)]\n",
    "    print(\"Argmax &\", \" & \".join(\n",
    "        [np.array2string(a, separator=\",\") for a in argmax]), r\"\\\\\")\n",
    "\n",
    "print(\"Single mean\")\n",
    "print_statistics_latex(M_single_mean)\n",
    "print(\"Single max\")\n",
    "print_statistics_latex(M_single_max)\n",
    "print(\"Z-score\")\n",
    "print_statistics_latex(M_fusion_z_score)\n",
    "print(\"Regression\")\n",
    "print_statistics_latex(M_fusion_regression)\n",
    "\n",
    "print(\"Fusion sign tests\")\n",
    "print(\"Z-score/T/Single-mean\")\n",
    "print(*sign_test(M_fusion_z_score, M_single_mean))\n",
    "print(\"Z-score/T/Single-max\")\n",
    "print(*sign_test(M_fusion_z_score, M_single_max))\n",
    "print(\"Regression/T/Single-mean\")\n",
    "print(*sign_test(M_fusion_regression, M_single_mean))\n",
    "print(\"Regression/T/Single-max\")\n",
    "print(*sign_test(M_fusion_regression, M_single_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veto evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _method(threshold, value):\n",
    "    def f(scores):\n",
    "        scores[scores < threshold] = value\n",
    "        return scores\n",
    "    return f\n",
    "\n",
    "def _veto_fusions(rls_training, rls_testing, Y_training, Y_testing, value):\n",
    "    models = [fusion_regression_training(\n",
    "        rl, Y_training)[0] for rl in rls_training]\n",
    "    rl_no_veto = fusion_regression(models, rls_testing)\n",
    "    baseline = evaluate_linking(rl_no_veto, Y_testing)[0]\n",
    "    y = []\n",
    "    for xi in x:\n",
    "        rl_veto = fusion_regression(\n",
    "            models, rls_testing, alter_scores=_method(xi, value))\n",
    "        m = evaluate_linking(rl_veto, Y_testing)\n",
    "        y.append(m[0])\n",
    "    y = np.array(y) - baseline\n",
    "    return y\n",
    "\n",
    "def _plot(rls_A, rls_B, value, c, x):\n",
    "    y1 = _veto_fusions(rls_A, rls_B, Y_A, Y_B, value)\n",
    "    plt.plot(x, y1, ls=\"dotted\", c=c, alpha=0.5)\n",
    "    y2 = _veto_fusions(rls_B, rls_A, Y_B, Y_A, value)\n",
    "    plt.plot(x, y2, ls=\"dashed\", c=c, alpha=0.5)\n",
    "    print(f\"Set to {value} & {np.max(y1):.2e}/{x[np.argmax(y1)]:.2f} & {np.max(y2):.2e}/{x[np.argmax(y2)]:.2f}\") \n",
    "    \n",
    "rls_A = rls_st_jean_A\n",
    "rls_B = rls_st_jean_B\n",
    "\n",
    "x = np.linspace(0.01, 0.25, 25)\n",
    "values = [0, -1, -len(rls_A), -np.inf]\n",
    "\n",
    "custom_lines = [\n",
    "    Line2D([0], [0], color=\"k\", lw=2, ls=\"dotted\"),\n",
    "    Line2D([0], [0], color=\"k\", lw=2, ls=\"dashed\"),\n",
    "] + [Line2D([0], [0], color=f\"C{i}\", lw=2) for i in range(len(values))]\n",
    "labels = [\"Train A / Test B\", \"Train B / Test A\"] + \\\n",
    "    [f\"Set {str(v)}\" for v in values]\n",
    "\n",
    "plt.figure(figsize=(6, 4), dpi=200)\n",
    "for i, value in enumerate(values):\n",
    "    _plot(rls_A, rls_B, value, f\"C{str(i)}\", x)\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"AP gain\")\n",
    "plt.legend(custom_lines, labels)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/veto.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance over rank\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rank_list = compute_links([X_token_brunet, 0, 500, True, 1e-1, distances.manhattan])\n",
    "print(len(rank_list))\n",
    "print(rank_list[0:10])\n",
    "plt.figure(figsize=(4, 3), dpi=200)\n",
    "plt.scatter(range(len(rank_list)), [r[-1] for r in rank_list], s=1, marker=\",\")\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/distance_over_rank.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1000\n",
    "plt.figure(figsize=(4, 3), dpi=200)\n",
    "\n",
    "min_ = 0\n",
    "max_ = 5\n",
    "zoom_factors = np.arange(min_, max_, 0.01)\n",
    "\n",
    "plt.rcParams[\"axes.prop_cycle\"] = plt.cycler(\n",
    "    \"color\", plt.cm.hsv(np.linspace(0, 1, len(zoom_factors))))\n",
    "\n",
    "for i in zoom_factors:\n",
    "    y = s_curves.sigmoid_reciprocal(c=i, r=0.5)(scale)\n",
    "    plt.plot(y, linewidth=0.2)\n",
    "\n",
    "cbar = plt.colorbar(plt.cm.ScalarMappable(\n",
    "    norm=colors.Normalize(min_, max_), cmap=\"hsv\"))\n",
    "cbar.set_label(\"c\")\n",
    "plt.xlabel(\"Corrresponding rank\")\n",
    "plt.ylabel(\"S-Curve weighting\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/s_curve_c.png\")\n",
    "\n",
    "plt.rcParams['axes.prop_cycle'] = plt.rcParamsDefault['axes.prop_cycle']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1000\n",
    "plt.figure(figsize=(4, 3), dpi=200)\n",
    "\n",
    "min_ = 0.1\n",
    "max_ = 0.9\n",
    "rs = np.arange(min_, max_, 0.001)\n",
    "\n",
    "plt.rcParams[\"axes.prop_cycle\"] = plt.cycler(\n",
    "    \"color\", plt.cm.hsv(np.linspace(0, 1, len(rs))))\n",
    "\n",
    "for ri in rs:\n",
    "    y = s_curves.sigmoid_reciprocal(r=ri)(scale)\n",
    "    plt.plot(y, linewidth=0.2)\n",
    "\n",
    "cbar = plt.colorbar(plt.cm.ScalarMappable(norm=colors.Normalize(min_, max_), cmap=\"hsv\"))\n",
    "cbar.set_label(\"r\")\n",
    "plt.xlabel(\"Corrresponding rank\")\n",
    "plt.ylabel(\"S-Curve weighting\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/s_curve_r.png\")\n",
    "\n",
    "plt.rcParams['axes.prop_cycle'] = plt.rcParamsDefault['axes.prop_cycle']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1000\n",
    "plt.figure(figsize=(4, 3), dpi=200)\n",
    "y = s_curves.sigmoid_reciprocal(r=0.85, c=4)(scale)\n",
    "plt.plot(y)\n",
    "plt.xlabel(\"Corrresponding rank\")\n",
    "plt.ylabel(\"S-Curve weighting\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/s_curve_example.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft-veto evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rls = rls_oxquarry\n",
    "Y = Y_oxquarry\n",
    "# rls = rls_brunet\n",
    "# Y = Y_brunet\n",
    "# rls = rls_st_jean\n",
    "# Y = Y_st_jean\n",
    "\n",
    "rl = fusion_z_score(rls)\n",
    "M_vanilla = evaluate_linking(rl, Y)[0]\n",
    "\n",
    "resolution = 21\n",
    "cs = np.linspace(1e-6, 20, resolution)\n",
    "rs = np.linspace(0.1, 0.9, resolution)\n",
    "print(cs)\n",
    "print(rs)\n",
    "\n",
    "c_r = np.array(list(itertools.product(cs, rs)))\n",
    "M_softveto = []\n",
    "\n",
    "for a, b in tqdm(c_r):\n",
    "    s_curve = s_curves.sigmoid_reciprocal(c=a, r=b)\n",
    "    rls_veto = [s_curves.soft_veto(rl, s_curve) for rl in rls]\n",
    "    rl = fusion_z_score(rls_veto)\n",
    "    M_softveto.append(evaluate_linking(rl, Y)[0])\n",
    "\n",
    "M_softveto = np.array(M_softveto).reshape((resolution, -1))\n",
    "M_gain = M_softveto - M_vanilla\n",
    "\n",
    "vmax = np.max(np.abs([np.min(M_gain), np.max(M_gain)]))\n",
    "\n",
    "print(f\"{np.max(M_gain):.2e} / {c_r[np.argmax(M_gain)]}\")\n",
    "\n",
    "plt.figure(figsize=(4,3), dpi=200)\n",
    "plt.scatter(x=c_r[:, 0], y=c_r[:, 1], c=M_gain, cmap=\"RdYlGn\", marker=\"s\",vmin=-vmax, vmax=vmax)\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(\"AP gain\")\n",
    "plt.xlabel(\"c\")\n",
    "plt.ylabel(\"r\")\n",
    "plt.xticks(np.linspace(np.min(cs), np.max(cs), 5))\n",
    "plt.yticks(np.linspace(np.min(rs), np.max(rs), 5))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/soft_veto.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average precision fusion gain relation with the rank lists diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "def compute_method_rank_list(rls):\n",
    "    s = len(rls)\n",
    "    ids = range(s)\n",
    "    d = []\n",
    "    for Ai, Bi in itertools.product(ids, ids):\n",
    "        A, B = rls[Ai], rls[Bi]\n",
    "        dist = rank_list_distance(A, B)\n",
    "        d.append(dist)\n",
    "    d = np.array(d).reshape((s, s))\n",
    "    return d\n",
    "\n",
    "def get_ranks(d, rls, Y, aggregation):\n",
    "    gain, dist_value = [], []\n",
    "    for ids in itertools.combinations(range(d.shape[0]), 2):\n",
    "        fusion_rl = [rls[id] for id in ids]\n",
    "        \n",
    "        m_single_rls = [evaluate_linking(rl, Y) for rl in fusion_rl]\n",
    "        m_single = aggregation(m_single_rls, axis=0)\n",
    "        m_fuse = evaluate_linking(fusion_z_score(fusion_rl), Y)\n",
    "        diff = m_fuse - m_single\n",
    "        \n",
    "        gain.append(diff[0])\n",
    "        dist_value.append(d[ids])\n",
    "        \n",
    "    return np.array(dist_value), np.array(gain)\n",
    "\n",
    "\n",
    "rls = rls_oxquarry\n",
    "Y = Y_oxquarry\n",
    "# rls = rls_brunet\n",
    "# Y = Y_brunet\n",
    "# rls = rls_st_jean\n",
    "# Y = Y_st_jean\n",
    "\n",
    "d = compute_method_rank_list(rls)\n",
    "print(d)\n",
    "\n",
    "def plot(aggregation):\n",
    "    plt.figure(figsize=(6,4), dpi=200)\n",
    "    wilcoxon, gain = get_ranks(d, rls, Y, aggregation)\n",
    "    reg = linregress(wilcoxon, gain)\n",
    "    slope, intercept, r_value, p_value, std_err = reg\n",
    "    print(slope, intercept)\n",
    "    print(f\"{r_value:.2f} & {p_value:.2e} & {std_err:.2e}\")\n",
    "    def f(x):\n",
    "        return slope * x + intercept\n",
    "    Xs = np.array([np.min(wilcoxon), np.max(wilcoxon)])\n",
    "    Ys = f(Xs)\n",
    "    plt.scatter(wilcoxon, gain, label=\"2 rank lists z-score fusion\")\n",
    "    plt.plot(Xs, Ys, label=\"Linear regression\")\n",
    "    plt.xlabel(r\"Kendall-$\\tau$ coefficient\")\n",
    "    plt.ylabel(\"AP gain\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"img/rank_list_correlation_{aggregation.__name__}.png\", facecolor=\"w\")\n",
    "    \n",
    "plot(np.mean)\n",
    "plot(np.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with fused rank lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mc = []\n",
    "\n",
    "for name, rl, Y in datasets_zscores:\n",
    "    f1_a = best_clustering(rl, \"single\", Y)\n",
    "    f1_b = best_clustering(rl, \"average\", Y)\n",
    "    f1_c = best_clustering(rl, \"complete\", Y)\n",
    "    Mc.append([f1_a, f1_b, f1_c])\n",
    "\n",
    "    print(f\"{name} & {f1_a:.2f} & {f1_b:.2f} & {f1_c:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mc = np.array(Mc)\n",
    "print(Mc.shape)\n",
    "\n",
    "print(np.mean(Mc, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette-based clustering evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkages = [\n",
    "    \"single\",\n",
    "    \"average\",\n",
    "    \"complete\",\n",
    "]\n",
    "\n",
    "# alpha = 0\n",
    "alpha = -0.2\n",
    "\n",
    "M = []\n",
    "\n",
    "for (_, rl, Y), linkage in itertools.product(datasets_zscores, linkages):\n",
    "    labels, silhouette_scores = silhouette_based_clustering(rl, linkage=linkage, alpha=alpha)\n",
    "    m = evaluate_clustering(Y, labels)\n",
    "    M.append(m)\n",
    "    \n",
    "M = np.array(M).reshape(len(datasets_zscores), len(linkages), -1)\n",
    "print(M[:,:,(0,-1)])\n",
    "print(np.abs(M).mean(axis=(0))[:,(0,-1)])\n",
    "# print(np.abs(M).mean(axis=(1))[:,(0,-1)])\n",
    "# print(np.abs(M).mean(axis=(0,1))[[0, -1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution-based clustering evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mc = []\n",
    "\n",
    "dts = []\n",
    "\n",
    "for name, rl, Y in datasets_zscores:\n",
    "    dts.append(dist_thresh_two_beta(rl, Y))\n",
    "    \n",
    "print(dts)\n",
    "\n",
    "for (training_id, dt), (name, rl, Y) in itertools.product(enumerate(dts), datasets_zscores):\n",
    "    labels_a = clustering_at_dist_thresh(rl, \"single\", dt)\n",
    "    m_a = evaluate_clustering(Y, labels_a)\n",
    "\n",
    "    labels_b = clustering_at_dist_thresh(rl, \"average\", dt)\n",
    "    m_b = evaluate_clustering(Y, labels_b)\n",
    "\n",
    "    labels_c = clustering_at_dist_thresh(rl, \"complete\", dt)\n",
    "    m_c = evaluate_clustering(Y, labels_c)\n",
    "\n",
    "    print(f\"{training_id} & {name} & {m_a[[0,-1]]} & {m_b[[0,-1]]} & {m_c[[0,-1]]}\")\n",
    "    \n",
    "    Mc.append([m_a, m_b, m_c])\n",
    "\n",
    "Mc = np.array(Mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mc = Mc.reshape((-1, len(datasets_rls), 3, 4))\n",
    "print(Mc.shape)\n",
    "\n",
    "print(\"Single\")\n",
    "print(Mc[:,:,0,[0, -1]])\n",
    "print(np.mean(Mc[:,:,0,[0, -1]],axis=(0)))\n",
    "print(np.mean(Mc[:,:,0,[0, -1]],axis=(1)))\n",
    "print(np.mean(Mc[:,:,0,[0, -1]],axis=(0,1)))\n",
    "\n",
    "print(\"Average\")\n",
    "print(Mc[:,:,1,[0, -1]])\n",
    "print(np.mean(Mc[:,:,1,[0, -1]],axis=(0)))\n",
    "print(np.mean(Mc[:,:,1,[0, -1]],axis=(1)))\n",
    "print(np.mean(Mc[:,:,1,[0, -1]],axis=(0,1)))\n",
    "\n",
    "print(\"Complete\")\n",
    "print(Mc[:,:,2,[0, -1]])\n",
    "print(np.mean(Mc[:,:,2,[0, -1]],axis=(0)))\n",
    "print(np.mean(Mc[:,:,2,[0, -1]],axis=(1)))\n",
    "print(np.mean(Mc[:,:,2,[0, -1]],axis=(0,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression-based clustering evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mc = []\n",
    "\n",
    "for (name_training, rl_training, Y_training), (name_testing, rl_testing, Y_testing) in itertools.product(datasets_zscores, datasets_zscores):\n",
    "    dt = dist_thresh_logistic_regression(rl_training, Y_training, rl_testing)\n",
    "\n",
    "    labels_a = clustering_at_dist_thresh(rl_testing, \"single\", dt)\n",
    "    m_a = evaluate_clustering(Y_testing, labels_a)\n",
    "\n",
    "    labels_b = clustering_at_dist_thresh(rl_testing, \"average\", dt)\n",
    "    m_b = evaluate_clustering(Y_testing, labels_b)\n",
    "\n",
    "    labels_c = clustering_at_dist_thresh(rl_testing, \"complete\", dt)\n",
    "    m_c = evaluate_clustering(Y_testing, labels_c)\n",
    "\n",
    "    Mi = [m_a, m_b, m_c]\n",
    "\n",
    "    print(f\"{name_training} & {name_testing} & {m_a[[0,-1]]} & {m_b[[0,-1]]} & {m_c[[0,-1]]}\")\n",
    "    \n",
    "    Mc.append(Mi)\n",
    "    \n",
    "Mc = np.array(Mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mc = Mc.reshape((-1, len(datasets_rls), 3, 4))\n",
    "print(Mc.shape)\n",
    "\n",
    "print(\"Single\")\n",
    "print(Mc[:,:,0,[0, -1]])\n",
    "print(np.mean(Mc[:,:,0,[0, -1]],axis=(0)))\n",
    "print(np.mean(Mc[:,:,0,[0, -1]],axis=(1)))\n",
    "print(np.mean(Mc[:,:,0,[0, -1]],axis=(0,1)))\n",
    "\n",
    "print(\"Average\")\n",
    "print(Mc[:,:,1,[0, -1]])\n",
    "print(np.mean(Mc[:,:,1,[0, -1]],axis=(0)))\n",
    "print(np.mean(Mc[:,:,1,[0, -1]],axis=(1)))\n",
    "print(np.mean(Mc[:,:,1,[0, -1]],axis=(0,1)))\n",
    "\n",
    "print(\"Complete\")\n",
    "print(Mc[:,:,2,[0, -1]])\n",
    "print(np.mean(Mc[:,:,2,[0, -1]],axis=(0)))\n",
    "print(np.mean(Mc[:,:,2,[0, -1]],axis=(1)))\n",
    "print(np.mean(Mc[:,:,2,[0, -1]],axis=(0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
