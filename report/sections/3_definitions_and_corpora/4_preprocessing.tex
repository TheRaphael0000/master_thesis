\section{Pre-processing}

A pre-processing of the data is realized to prepare it for the next steps.
The pre-processing is in two parts.
The first part is specific to each corpus, and the second is the same for every corpus.

Oxquarry and Brunet are already tokenized such that every token are on a separated line.
Additionally, Brunet have a lemma representation of the texts.
This representation is contained in a separated file, each lemma is on a separated line.
For these two corpora, a simple script is used to parse the file and creates a words vector for each document.

St-Jean is also already tokenized with one token per line approach.
Unlike the Brunet dataset, each document have its three representation (token, lemma and POS) in the same file.
The three representation are separated by a comma on each line.
There are a few additional preprocessing needed for this corpus.

When the word \textit{des} (equivalent to a plural \textit{the} in French) is encounter, the tokenizer used to create the St-Jean files created two lines for this word since it can be lemmatized into either \textit{de} (\textit{some/any}) or \textit{le} (\textit{the}).
To avoid having these words weighted twice, only the first line is kept.

St-Jean also have another specificity, it contains both the numerical representation of numbers and the textual representation.
For example, the number \textit{89} is written in St-Jean as:
\begin{verbatim}
<Nombre 89>,<>,<>
quatre,quatre,72
vingt,vingt,72
neuf,neuf,72
<Fin nombre>,<>,<>
\end{verbatim}
The first line is a tag which contain the actual number found in the text, the three next lines are the words used to spell this number in French (\textit{quatre}: $4$, \textit{vingt}: $20$, \textit{neuf}: $9$, $4 \cdot 20 + 9 = 89$) and the last lines is a tag to escape the number sequence.
Only the numerical representation is kept, in the example \textit{89}.
This type of numerical representation is also used for ordinal number, such as \textit{7e} (7th):
\begin{verbatim}
<Nombre 7e>,<>,<>
septième,septième,72
<Fin nombre>,<>,<>
\end{verbatim}
When the number is already written in full letters in the text, the parser did not tokenize it this way, only one line is created.

The first two line of each document are ignored for St-Jean since they contain metadata for the document.
The following information can be found: the number of tokens in the document, the name of the collection and the document ID written is full text.
Some inconsistencies on these two lines have been corrected manually.
For example, in some files, only one of these lines was available.

For Oxquarry, Brunet and St-Jean, the authors of each document are contained in a single text file.
Each line contains the author of the document with an ID equivalent to the line number, e.g. author on line 1 is for document 1, author on line 2 is for document 2, etc.

For the PAN16 corpus, there is no document tokenization available.
Plain text files are given.
A simple tokenization is realized which consider every punctuation symbols (POSIX punctuation symbols class), line breaks and spaces (POSIX spaces class) as a separator for the different tokens.
Since they are written in multiple languages, no further rules are applied to tokenize more effectively the texts.
The problem with this tokenization method is that it removes the punctuation symbols, which carry information.

The general preprocessing applied on every document of every corpus is to encode every text with only lower case ASCII characters.
By doing so, every diacritic are removed, e.g. the word \textit{École} (school in French) is converted to \textit{ecole}.
This can create ambiguity in French but was ignored.
Here is an example of ambiguity created with this method: the words \textit{jeune} (\textit{young} in French) and \textit{jeûne} (\textit{fasting} in French).
This method is not used for the PAN dataset, since the Greek alphabet is not in the ASCII character set.
