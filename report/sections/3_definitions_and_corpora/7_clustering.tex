\section{Clustering}

\subsection{Definition and Examples}

The clustering aim to group items such that the most similar items are in the same group.
These groups of similar items are called clusters.

Example~\ref{ex:clustering} show the clustering task for 2D points.
The points are grouped according to their distance.

For the case of authorship clustering, finding the authorship function can be useful (Definition~\ref{def:authorship} in Section~\ref{sec:definitions}).
But this task can be hard since the real author (ground truth labels) are generally not available.
Without example, finding the authorship function is not possible.

For the authorship clustering, we only aim to group similar texts instead of finding the actual authors.
This task can be solved by finding the relevant set (Definition~\ref{def:relevant_set} in Section~\ref{sec:definitions}).

To find clusters from the relevant set, a simple method is to:
Consider each pair of documents in the relevant set as: \textit{in the same cluster}.
The clusters are created using transitivity.

One main issue arise with this method.
If the relevant set is not perfect (documents pairs with actual different authors are in the relevant set), the two authors will be in the same cluster.
Noisy relevant set can quickly be considered as a single cluster with this method.

Chapter~\ref{sec:clustering} will introduce and evaluate methods using rank lists to create the clusters instead of the relevant set.

\begin{example}
    \centering
    \caption{Clustering with 2D points}
    \label{ex:clustering}

    \begin{subexample}{\linewidth}
      \centering
      \subcaption{Unlabeled data}
      \includegraphics[width=0.9\linewidth]{img/clustering_example_1.png}
    \end{subexample}

    \vspace{0.5cm}

    \begin{subexample}{\linewidth}
      \centering
      \subcaption{A possible good clustering result}
      \includegraphics[width=0.9\linewidth]{img/clustering_example_2.png}
    \end{subexample}
\end{example}

\subsection{Clustering Evaluation Metrics \label{sec:clustering_evaluation_meterics}}

The methods introduced in this chapter are to evaluate and compare clustering models.
Therefore, the ground truth labels are required.
The metrics used are the BCubed family (also called $B^3$, in this report)~\cite{bcubed}.

BCubed has shown to satisfy the four following important constraints when evaluating clusterings~\cite{bcubed}~:

\begin{enumerate}
  \item
  \textit{Cluster Homogeneity} : different authors should be in the different clusters
  \item
  \textit{Cluster Completeness} : same authors should belong to the same cluster
  \item
  \textit{Rag Bag constraint} : noisy or miscellaneous authors should be in the same cluster and not in \textit{healthy} clusters
  \item
  \textit{Cluster size vs quantity constraints} : favour large cluster
\end{enumerate}

These metrics are used in the clustering task at PAN @ CLEF~\cite{pan16}.
In addition to the BCubed family, this study introduce a simple metric for evaluating the clustering called \textit{cluster difference}.

\begin{definition}[Correctness~\cite{bcubed}]
  The $BCubed$ metric family is based on the following \textit{Correctness} principle.
  Let L(e) and C(e) be the author and the cluster of an element e.
  The Correctness is following the biconditional condition on the author and cluster equality (biconditional: $A \Longleftrightarrow B \equiv (A \land B) \lor (\neg A \land \neg B)$).
  \begin{gather*}
    Correctness(e, e') = \\
    \begin{cases}
      1, & if (L(e) = L(e')) \Longleftrightarrow (C(e) = C(e'))\\
      0, & otherwise
    \end{cases}
  \end{gather*}
  In other terms, the correctness has a value of 1 (100\% correct) if the two elements are in the both in the same cluster and have the same author OR both are in a different cluster and have a different author.
\end{definition}

\begin{definition}[$BCubed$ Precision~\cite{bcubed}]
  The $BCubed$ Precision correspond to the average of correctness for all elements on the average of all element such that \textbf{their clusters are the same}.
  \begin{gather*}
    B^3_{precision} = \text{Avg}_{e}[\text{Avg}_{e' C(e)=C(e')}[Correctness(e, e')]]
  \end{gather*}
\end{definition}

\begin{definition}[$BCubed$ Recall~\cite{bcubed}]
  The $BCubed$ Recall correspond to the average of correctness for all elements on the average of all element such that \textbf{their authors are the same}.
  \begin{gather*}
    B^3_{recall} = \text{Avg}_{e}[\text{Avg}_{e' L(e)=L(e')}[Correctness(e, e')]]
  \end{gather*}
\end{definition}

\begin{definition}[$BCubed F_1$ Score~\cite{bcubed}]
  $BCubed F_1$ Score uses the harmonic mean between the $B^3_{precision}$ and $B^3_{recall}$.
  \begin{gather*}
    B^3_{F_1} =
    2 \cdot \frac{B^3_{precision} \cdot B^3_{recall}}
    {B^3_{precision} + B^3_{recall}}
  \end{gather*}
  The $F_\beta$ measures, not shown here, provide a parametric way to represent with a single value the two counterbalancing measures in this case, the $B^3_{F_1}$ is computed using the $F$ measures with $\beta = 1$ and the $B^3_{precision}$ and $B^3_{recall}$ scores.
\end{definition}

\begin{definition}[Cluster difference]
  This metric aim to evaluate if the clustering found the right number of cluster.
  The cluster difference is the number of cluster found $p$ minus the actual number of cluster $k$ (number of distinct authors in the corpus).
  \begin{gather*}
    Cluster_{diff} = p - k
  \end{gather*}
  A positive value indicates an overestimation of the real number of cluster, a negative value indicate the underestimation.
  Zero indicate that the right number of cluster was found.
  This value can also be used to summarize if the $B^3_{recall}$ is greater or not than the $B^3_{precision}$.
  A positive Cluster diff should indicate a larger $B^3_{precision}$ than $B^3_{recall}$, and vice versa.

  This value can be normalized by the number of documents N, which correspond to the difference of the r ratios.
  This is useful when comparing corpora results with different number of clusters.
  \begin{gather*}
    r_{diff} = \frac{p}{N} - \frac{k}{N} = \frac{p - k}{N}
  \end{gather*}
  As stated in the PAN16 evaluation campaign paper, estimating correctly the number of clusters and the r ratio is a first step to produce a good clustering~\cite{pan16}.
\end{definition}
