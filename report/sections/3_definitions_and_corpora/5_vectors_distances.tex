\section{Vectors Distances \label{sec:vectors_distances}}

To be able to compare vectors, different metrics can be used depending on the usage.
Usually, two types of metrics for vector comparison can be used: Vector similarities and vector distances.
The first yield a large value when the two vectors are similar, and the second a small value when they are closely related.
Definitions~\ref{def:manhattan} to~\ref{def:jeffrey_divergence} are one of the few most common $L^1$/$L^2$/$cosine$-based distances. We used the variable $n$ to indicate the vectors size.

\begin{definition}[Manhattan distance\label{def:manhattan}]
  \begin{gather*}
    dist_{Manhattan}(A, B) = \sum_{i=1}^{n} |a_i - a_i|
  \end{gather*}
  The Manhattan distance is also known as the $L^1$ distance measure.

  Example:

  Let $A$ and $B$ be two same size vectors.

  \begin{gather*}
    \begin{aligned}
      A &= \left[-2, 3, -5, 7, -11, 13 \right] \\
      B &= \left[1, 2, 4, 8, 16, 32 \right]
    \end{aligned}
  \end{gather*}

  The Manhattan distance is computed as follows:

  \begin{gather*}
    \begin{aligned}
      dist_{Manhattan}(A, B) =& |-2 - 1| + |3 - 2| \\
                             +& |-5 - 4| + |7 - 8| \\
                             +& |-11 - 16| + |13 - 32| \\
                             =& |-3| + |1| + |-9| \\
                             +& |-1| + |-27| + |-19| \\
                             =& 60
    \end{aligned}
  \end{gather*}
\end{definition}

\begin{definition}[Tanimoto distance]
  \begin{gather*}
    dist_{Tanimoto}(A, B) = \frac{dist_{Manhattan}(A, B)}{\sum_{i=1}^{n} \mathrm{max}(a_i, b_i)}
  \end{gather*}
  Tanimoto distance is a $L^1$-based, it's a component-wise normalized version of the Manhattan distance.
\end{definition}

\begin{definition}[Euclidean distance]
  \begin{gather*}
    dist_{Euclidian}(A, B) = \sqrt{\sum_{i=1}^{n}(a_i - b_i)^2}
  \end{gather*}
  The $L^2$ distance.
\end{definition}

\begin{definition}[Matusita distance]
  \begin{gather*}
    \begin{split}
      dist_{Matusita}(A, B) &= dist_{Euclidian}(A', B') \\
      \text{with }A' &= \sqrt{A}\text{ and }B' = \sqrt{B}
    \end{split}
  \end{gather*}
  An $L^2$-based distance, using the square root of the input vectors with the Euclidean distance.
\end{definition}

\begin{definition}[Clark distance~\cite{kocher_verification}]
  \begin{gather*}
    dist_{Clark}(A, B) = \sqrt{\sum_{i=1}^{n}\left(\frac{|a_i - b_i|}{a_i + b_i}\right)^2}
  \end{gather*}
  An $L^2$-based distance.
\end{definition}

\begin{definition}[Cosine distance]
  \label{def:cosine_dist}
  The cosine distance is an inner product based distance.
  The inner product is defined as:

  \begin{gather*}
    \langle A, B \rangle = \sum_{i=1}^{n} a_i b_i = a_1 b_1 + a_2 b_2 + \cdots + a_n b_n
  \end{gather*}

  To compute the Cosine distance, first the cosine similarity must be computed.
  \begin{gather*}
    sim_{Cosine}(A, B) = \frac{\langle A, B \rangle}{\sqrt{\langle A , A \rangle}\sqrt{\langle B, B \rangle}}
  \end{gather*}
  The cosine similarity is ranged between -1 and 1.
  With 1 being exactly the same, -1 the total opposite and 0 orthogonal.
  \begin{gather*}
    dist_{Cosine}(A, B) = 1 - sim_{Cosine}(A, B)
  \end{gather*}
  \begin{gather*}
    dist_{Angular\_cosine}(A, B) = \frac{cos^{-1}\left( sim_{Cosine}(A, B) \right)}{\pi}
  \end{gather*}
  These measures are based on the inner product.

  In this study, the only inner product based metric used is the cosine distance.
\end{definition}

\begin{definition}[Kullback-Leibler divergence]
  \begin{gather*}
    dist_{kld}(A\ ||\ B) = \sum_{i=1}^{n} a_i \cdot \log(\frac{a_i}{b_i})
  \end{gather*}
  Kullback-Leibler divergence use the principle of relative entropy.
\end{definition}

\begin{definition}[Jeffrey divergence~\cite{kocher_verification}\label{def:jeffrey_divergence}]
  \begin{gather*}
    dist_{j\_divergence}(A\ ||\ B) = \sum_{i=1}^{n} (a_i - b_i) \cdot \log(\frac{a_i}{b_i})
  \end{gather*}
  It represents the difference between two probability distribution.
\end{definition}
