\subsection{Rank lists}

Rank lists are used to order objects such that the most interesting object is at the top and every subsequent object become less interesting.
In information retrieval systems, rank lists are used to order the results from the most relevant to the user's query to the least relevant.
For the authorship verification problem, the rank list can also be used.

\subsubsection{Rank lists definition and example}

\begin{definition}[Ranked list for authorship verification \label{def:rank_list}]
  \begin{equation}
    L = (\left[(X_a, X_b): Score(X_a, X_b)\right] | X_a \neq X_b \forall (X_a, X_b))
  \end{equation}
  \begin{equation}
    |L| = \frac{N \cdot (N - 1)}{2}
  \end{equation}
\end{definition}

A ranked list for the authorship verification problem is an ordered list containing document pairs and a score for the pair.
In most cases, the rank list contain every possible pairs of documents.

The ranked list is ordered by the score, such that the most similar document pair are at the top of the list.
When the scoring function is based on a distance metrics, the rank list is sorted in increasing order.
For the scoring function based on similarity, the rank list is sorted in decreasing order.
The most similar documents are the most likely they are written by the same author.
Thus, the top ranks should contain pairs of documents written by the same author.

The computational cost for computing a rank list is $\frac{n \cdot (n - 1)}{2}$ multiplied by the computational cost of the score function.
Since the complexity of the score function is lower than $O(n^2)$, the $O(n^2)$ complexity remain.
The space complexity is also $O(n^2)$, for each document pair, the pair and the score have to be stored.

Example~\ref{ex:rank_list} show a simple example using two-dimensional vectors and the Manhattan distance (presented in Section~\ref{sec:vectors_distances}) to create a rank list.

\begin{example}
  \centering
  \caption{Rank list computation using two-dimensional vectors and the Manhattan distance}
  \label{ex:rank_list}

  \begin{subexample}{\linewidth}
    \centering
    \subcaption{List of two-dimensional vectors}
    \begin{tabular}{l r r}
      \toprule
      Vector ID & Vector \\
      \midrule
      0 & $[0, 0]$ \\
      1 & $[1, 2]$ \\
      2 & $[4, 6]$ \\
      3 & $[1, 4]$ \\
      \bottomrule
    \end{tabular}
  \end{subexample}

  \vspace{0.5cm}

  \begin{subexample}{\linewidth}
    \centering
    \subcaption{Pairwise manhanttan distances}
    \begin{tabular}{l l}
      \toprule
      Vector Pair IDs & $dist_{Manhattan}(A, B)$ \\
      \midrule
      (0, 1) & $|0-1| + |0-2| = 3$ \\
      (0, 2) & $|0-4| + |0-6| = 10$ \\
      (0, 3) & $|0-1| + |0-4| = 5$ \\
      (1, 2) & $|1-4| + |2-6| = 7$ \\
      (1, 3) & $|1-1| + |2-4| = 2$ \\
      (2, 3) & $|4-1| + |6-4| = 5$ \\
      \bottomrule
    \end{tabular}
  \end{subexample}

  \vspace{0.5cm}

  \begin{subexample}{\linewidth}
    \centering
    \subcaption{Ordered rank list by distances}
    \begin{tabular}{l c r}
      \toprule
      Rank & Vector Pair IDs & $dist_{Manhattan}(A, B)$ \\
      \midrule
      1st   & (1, 3) & $2$ \\
      2nd   & (0, 1) & $3$ \\
      3-4rd & (2, 3) & $5$ \\
      3-4th & (0, 3) & $5$ \\
      5th   & (1, 2) & $7$ \\
      6th   & (0, 2) & $10$ \\
      \bottomrule
    \end{tabular}
  \end{subexample}

\end{example}


\subsubsection{Rank lists evaluation metrics \label{sec:rl_eval}}

In order to know the quality of a rank list, multiple rank list evaluation metrics are used and presented in this section.
Definitions in these sections are adapted versions of the ones from Kocher and Savoy~\cite{kocher_linking}.
The presented metrics are also well know in the authorship verification and the information retrieval field.
Example~\ref{ex:rank_list_eval} show a example for each metric.

\begin{definition}[Relevant link~\cite{kocher_linking}]
  A relevant link is a link in the relevant set.
  The relevant set contain every document pair written by the same author, see Definition~\ref{def:relevant_set}.
  \begin{equation}
    relevant(l_i) =
    \begin{cases}
      1, & if\ l_i \in R \\
      0, & otherwise
    \end{cases}
  \end{equation}
\end{definition}

\begin{definition}[Precision@k~\cite{kocher_linking}]
  The precision@k is a function which take a positive integer k, with k < |L|
  \begin{equation}
    precision(k) = \frac{1}{k} \sum_{j=1}^{k} relevant(j)
  \end{equation}
\end{definition}

\begin{definition}[High precision~\cite{kocher_linking}]
  The high precision (HPrec) represent the maximal rank j in the rank list such that the precision is still 100\%.
  \begin{equation}
    HPrec = \max\{i \in \mathbf{N} | precision(i) = 1\}
  \end{equation}
  This value is in the range $\left[0, |R|\right]$.
  $0$ means the first pair in the rank list is incorrect.
  $|R|$ means every true links are ranked in the top part of the rank list.
\end{definition}

\begin{definition}[R-Precision~\cite{kocher_linking}]
  The R-Precision (RPrec) is the precision in the rank list at rank |R| (Precision@r).
  With R being the relevant set (Definition~\ref{def:relevant_set}).
  \begin{equation}
    RPrec = precision(|R|)
  \end{equation}
  The RPrec value is in the range $\left[0, 1\right]$.
  With 0 mean every link in the first $|R|$-ranks are not in the relevant set.
  And 1, every link in the first $|R|$-ranks are in the relevant set.
\end{definition}

\begin{definition}[Average Precision (AP)]
  The mean over the precision@k each time a relevant link is retrieved.
  The average precision can be considered as an approximation of the area under the precision-recall curve.
  \begin{equation}
    AveragePrecision = \frac{1}{|R|} \sum_{j=1}^{|L|} precision(j) \cdot relevant(j)
  \end{equation}
\end{definition}

\begin{example}
  \centering
  \caption{Rank list evaluation example}
  \label{ex:rank_list_eval}


  \begin{subexample}{\linewidth}
  \subcaption{Documents, authorship and rank list}

  Suppose that a corpus contain 4 documents (documents id: 0, 1, 2, 3).
  Documents 0, 1 and 3 are written by the same author A.
  Document 2 is written by author B.

  The following relevant set $R$ and non-relevant set $\bar{R}$ can be computed using these informations.

  $R = \{(0, 1), (0, 3), (1, 3) \}$

  $\bar{R} = \{(0, 2), (1, 2), (2, 3) \}$

  $|L| = |R| \cup |\bar{R}| = 6$

  Suppose the following rank list order for the pairs :
  $((1, 3), (0, 1), (2, 3), (0, 3), (1, 2), (0, 2))$
  \end{subexample}

  \vspace{0.5cm}

  \begin{subexample}{\linewidth}
    \centering
    \subcaption{Precision@k}
    \begin{tabular}{l c c c}
      \toprule
      Rank  & Pair IDs & Pair $\in R$ & Precision@k\\
      \midrule
      1st   & (1, 3)   & Yes  & 1.00 \\
      2nd   & (0, 1)   & Yes  & 1.00 \\
      3rd   & (2, 3)   & No   & 0.66 \\
      4th   & (0, 3)   & Yes  & 0.75 \\
      5th   & (1, 2)   & No   & 0.60 \\
      6th   & (0, 2)   & No   & 0.50 \\
      \bottomrule
    \end{tabular}
  \end{subexample}

  \vspace{0.5cm}

  \begin{subexample}{\linewidth}
    \subcaption{High precision (HPrec)}
    \begin{equation}
      \begin{split}
        HPrec &= \max\{i \in \mathbf{N} | precision(i) = 1\} \\
              &= \max\{1, 2\} = 2 \\
      \end{split}
    \end{equation}
  \end{subexample}

  \vspace{0.5cm}

  \begin{subexample}{\linewidth}
    \subcaption{R-Precision (RPrec)}
    \begin{equation}
      \begin{split}
        RPrec = precision(|R|) = precision(3) = 0.66
      \end{split}
    \end{equation}
  \end{subexample}

  \vspace{0.5cm}

  \begin{subexample}{\linewidth}
    \subcaption{Average Precision (AP)}
    \begin{equation}
      \begin{split}
        AveragePrecision &= \frac{1}{|R|} \sum_{j=1}^{|L|} precision(j) \cdot relevant(j) \\
                         &= \frac{1}{3} \sum_{j=1}^{6} precision(j) \cdot relevant(j) \\
                         &= \frac{1}{3} ( 1.00 \cdot 1 + 1.00 \cdot 1 + 0.66 \cdot 0 \\
                         &+ 0.75 \cdot 1 + 0.60 \cdot 0 + 0.50 \cdot 0) \\
                         &= \frac{1}{3} (1.00 + 1.00 + 0.75) = 0.92 \\
      \end{split}
    \end{equation}
  \end{subexample}

\end{example}
