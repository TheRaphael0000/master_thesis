\section{Rank lists}

Rank lists are used to order objects such that the most interesting object is at the top and every subsequent object become less interesting.
In information retrieval systems, rank lists are used to order the results from the most relevant to the user's query to the least relevant.
For the authorship verification problem, the rank list can also be used.

\subsection{Definition and Example}

\begin{definition}[Ranked list for authorship verification \label{def:rank_list}]
  \begin{gather*}
      \begin{split}
        L = (\left[(X_a, X_b): Score(X_a, X_b)\right] | X_a \neq X_b \forall (X_a, X_b))
      \end{split}
  \end{gather*}
  \begin{gather*}
    |L| = \frac{N \cdot (N - 1)}{2}
  \end{gather*}
\end{definition}

A ranked list for the authorship verification problem is an ordered list containing document pairs and a score for the pair.
These pairs are also called links.
In most cases, the rank list contain every possible pairs of documents.

The ranked list is ordered by the score, such that the most similar document pair are at the top of the list.
When the scoring function is based on a distance metrics, the rank list is sorted in increasing order.
For the scoring function based on similarity, the rank list is sorted in decreasing order.
The most similar documents pairs are the most likely written by the same author.
Thus, the top ranks should contain pairs of documents written by the same author.

The computational cost to compute a rank list is $\frac{n \cdot (n - 1)}{2}$ multiplied by the computational cost of the scoring function.
Since the complexity of the score function is lower than $O(n^2)$, the $O(n^2)$ complexity remain.
The space complexity is also $O(n^2)$, for each document pair, the pair and the score have to be stored.

Example~\ref{ex:rank_list} show the creation of a rank list using two-dimensional vectors and the Manhattan distance (presented in Section~\ref{sec:vectors_distances}).

\begin{example}
  \centering
  \caption{Rank list computation using two-dimensional vectors and the Manhattan distance}
  \label{ex:rank_list}

  \begin{subexample}{\linewidth}
    \centering
    \subcaption{List of two-dimensional vectors}
    \begin{tabular}{l l}
      \toprule
      Vector ID & Vector \\
      \midrule
      0 & $[0, 0]$ \\
      1 & $[1, 2]$ \\
      2 & $[4, 6]$ \\
      3 & $[1, 4]$ \\
      \bottomrule
    \end{tabular}
  \end{subexample}

  \vspace{0.5cm}

  \begin{subexample}{\linewidth}
    \centering
    \subcaption{Pairwise Manhattan distances}
    \begin{tabular}{l l}
      \toprule
      Vector Pair IDs & $dist_{Manhattan}(A, B)$ \\
      \midrule
      (0, 1) & $|0-1| + |0-2| = 3$ \\
      (0, 2) & $|0-4| + |0-6| = 10$ \\
      (0, 3) & $|0-1| + |0-4| = 5$ \\
      (1, 2) & $|1-4| + |2-6| = 7$ \\
      (1, 3) & $|1-1| + |2-4| = 2$ \\
      (2, 3) & $|4-1| + |6-4| = 5$ \\
      \bottomrule
    \end{tabular}
  \end{subexample}

  \vspace{0.5cm}

  \begin{subexample}{\linewidth}
    \centering
    \subcaption{Ordered rank list by distances}
    \begin{tabular}{l c r}
      \toprule
      Rank & Vector Pair IDs & $dist_{Manhattan}(A, B)$ \\
      \midrule
      1st   & (1, 3) & $2$ \\
      2nd   & (0, 1) & $3$ \\
      3-4rd & (2, 3) & $5$ \\
      3-4th & (0, 3) & $5$ \\
      5th   & (1, 2) & $7$ \\
      6th   & (0, 2) & $10$ \\
      \bottomrule
    \end{tabular}
  \end{subexample}
\end{example}

\subsection{Evaluation Metrics \label{sec:rl_eval}}

In order to know the quality of a rank list, multiple rank list evaluation metrics are used and presented in this section.
Definitions in these sections are adapted versions of the ones from Kocher and Savoy~\cite{kocher_linking}.
The presented metrics are also well know in the authorship verification and the information retrieval field.

The average precision, the R-Precision and the High Precision are strongly correlated.
Thus, for this study, on some experiment, only the average precision is computed.

Example~\ref{ex:rank_list_eval} showcase each metric for the rank list evaluation.

\begin{definition}[Relevant link~\cite{kocher_linking}]
  A relevant link is a link in the relevant set.
  These are also called \textit{true links} in this study.
  The ones outside the relevant set are called \textit{false links}.
  The relevant set contains every document pair written by the same author, see Definition~\ref{def:relevant_set}.
  \begin{gather*}
    relevant(l_i) =
    \begin{cases}
      1, & if\ l_i \in R \\
      0, & otherwise
    \end{cases}
  \end{gather*}

\end{definition}

\begin{definition}[Precision@k~\cite{kocher_linking}~\cite{wiki_measures}]
  The precision@k is a function which take a positive integer k, with k < |L|
  \begin{gather*}
    precision(k) = \frac{1}{k} \sum_{j=1}^{k} relevant(j)
  \end{gather*}
\end{definition}

\begin{definition}[High precision~\cite{kocher_linking}]
  The high precision (HPrec) represent the maximal rank $j$ in the rank list such that the precision is still 100\%.
  \begin{gather*}
    HPrec = \max\{i \in \mathbf{N} | precision(i) = 1\}
  \end{gather*}
  This metric is in the range $\left[0, |R|\right]$.
  $HPrec=0$ means the first pair in the rank list is incorrect.
  $HPrec=|R|$ means every true links are ranked in the top part of the rank list.
\end{definition}

\begin{definition}[R-Precision~\cite{kocher_linking}~\cite{wiki_measures}]
  The R-Precision (RPrec) is the precision in the rank list at rank |R| (Precision@r).
  With R being the relevant set (Definition~\ref{def:relevant_set}).
  \begin{gather*}
    RPrec = precision(|R|)
  \end{gather*}
  The RPrec value is in the range $\left[0, 1\right]$.
  With $RPrec=0$, every link in the first $|R|$-ranks are not in the relevant set.
  And $RPrec=1$, every link in the first $|R|$-ranks are in the relevant set.
\end{definition}

\begin{definition}[Average Precision (AP)~\cite{wiki_measures}]
  The mean over the precision@k each time a relevant link is retrieved.
  \begin{gather*}
    AP = \frac{1}{|R|} \sum_{j=1}^{|L|} precision(j) \cdot relevant(j)
  \end{gather*}
  The average precision can be considered as an approximation of the area under the precision-recall curve.
\end{definition}

\begin{example}
  \centering
  \caption{Rank list evaluation example}
  \label{ex:rank_list_eval}

  \begin{subexample}{\linewidth}
  \subcaption{Documents, authorship and rank list}

  Suppose that a corpus contain 4 documents.
  Documents 0, 1 and 3 are written by the same author A.
  Document 2 is written by author B.

  The following relevant set $R$ and non-relevant set $\bar{R}$ can be computed using this information.
  \begin{gather*}
    \begin{split}
            R =& \{(0, 1), (0, 3), (1, 3) \} \\
      \bar{R} =& \{(0, 2), (1, 2), (2, 3) \} \\
          |L| =& |R| \cup |\bar{R}| = 6
    \end{split}
  \end{gather*}

  Suppose that these links are in a rank list with the following order:
  \begin{gather*}
    \begin{split}
      ((1, 3), (0, 1), (2, 3), (0, 3), (1, 2), (0, 2))
    \end{split}
  \end{gather*}
  \end{subexample}

  \vspace{0.5cm}

  \begin{subexample}{\linewidth}
    \centering
    \subcaption{Precision@k}
    \begin{tabular}{l c c c}
      \toprule
      Rank  & Pair IDs & Pair $\in R$ & Precision@k\\
      \midrule
      1st   & (1, 3)   & Yes  & $1/1 = 1.00$ \\
      2nd   & (0, 1)   & Yes  & $2/2 = 1.00$ \\
      3rd   & (2, 3)   & No   & $2/3 = 0.66$ \\
      4th   & (0, 3)   & Yes  & $3/4 = 0.75$ \\
      5th   & (1, 2)   & No   & $3/5 = 0.60$ \\
      6th   & (0, 2)   & No   & $3/6 = 0.50$ \\
      \bottomrule
    \end{tabular}
  \end{subexample}

  \vspace{0.5cm}

  \begin{subexample}{\linewidth}
    \subcaption{High precision (HPrec)}
    \begin{gather*}
      \begin{split}
        HPrec &= \max\{i \in \mathbf{N} | precision(i) = 1\} \\
              &= \max\{1, 2\} = 2 \\
      \end{split}
    \end{gather*}
  \end{subexample}

  \vspace{0.5cm}

  \begin{subexample}{\linewidth}
    \subcaption{R-Precision (RPrec)}
    \begin{gather*}
      \begin{split}
        RPrec = precision(|R|) = precision(3) = 0.66
      \end{split}
    \end{gather*}
  \end{subexample}

  \vspace{0.5cm}

  \begin{subexample}{\linewidth}
    \subcaption{Average Precision (AP)}
    \begin{gather*}
      \begin{split}
        AP &= \frac{1}{|R|} \sum_{j=1}^{|L|} precision(j) \cdot relevant(j) \\
           &= \frac{1}{3} \sum_{j=1}^{6} precision(j) \cdot relevant(j) \\
           &= \frac{1}{3} ( 1.00 \cdot 1 + 1.00 \cdot 1 + 0.66 \cdot 0 \\
           &+ 0.75 \cdot 1 + 0.60 \cdot 0 + 0.50 \cdot 0) \\
           &= \frac{1}{3} (1.00 + 1.00 + 0.75) = 0.92 \\
      \end{split}
    \end{gather*}
  \end{subexample}

\end{example}

\subsection{Rank Lists Relationship with Distances' Matrix \label{sec:distances_matrix}}

When computing the rank lists, each link have its distance calculated and can be represented in a matrix.
In this matrix, each document represent a row and a column.
The elements of the matrix are the distances between the two.
For the non-commutative distances functions, the whole matrix is used.
For the commutative distances functions, only a triangle or symmetric matrix is required.

A transformation can be effectuated either from a rank list to a distances' matrix or from a distances' matrix to a rank list.
For some algorithm or computations, the distances' matrix representation is required and for others the rank list.

Example~\ref{ex:distances_matrix} show the two representations for the same distances.

\begin{example}
  \centering
  \caption{Distances matrix and Rank lists}
  \label{ex:distances_matrix}
  \begin{subexample}{\linewidth}
    \centering
    \subcaption{Rank list for distances' matrix in Example~\ref{ex:distances_matrix_mx}\label{ex:distances_matrix_rl}}
    \begin{tabular}{l c r}
      \toprule
      Rank & Vector Pair IDs & Distance \\
      \midrule
      1st   & (1, 3) & $2$ \\
      2nd   & (0, 1) & $3$ \\
      3-4rd & (2, 3) & $5$ \\
      3-4th & (0, 3) & $5$ \\
      5th   & (1, 2) & $7$ \\
      6th   & (0, 2) & $10$ \\
      \bottomrule
    \end{tabular}
  \end{subexample}

  \vspace{0.5cm}

  \begin{subexample}{\linewidth}
    \centering
    \subcaption{Distances' matrix for rank list in Example~\ref{ex:distances_matrix_rl}\label{ex:distances_matrix_mx}}
    \begin{tabular}{c|c c c c}
      \toprule
        & 0 & 1 & 2  & 3 \\
      \midrule
      0 & - & 3 & 10 & 5 \\
      1 & - & - & 7  & 2 \\
      2 & - & - & -  & 5 \\
      3 & - & - & -  & - \\
      \bottomrule
    \end{tabular}
  \end{subexample}
\end{example}

\subsection{Evaluation Comparison~\label{sec:evaluation_comparison}}

To be able to compare rank list evaluation, a gain strategy is used.
In this study, the gain is defined as the difference between two evaluations.
It can be either positive or negative.
A positive gain indicates that the rank list have better performances over another.
When the gain is negative, the rank list have worse results than the other one.

When multiple rank list need to be compared to one rank list, two simple aggregation methods are used.
One use the mean of the results and the second the maximal value.
These are called respectively Single-Mean and Single-Max.
These aggregations strategies can be used in conjunction with the gain definition to compare one rank list to multiple rank lists.

The fusion methods proposed in this study transform multiple rank lists into a single rank list.
Thus, having aggregation strategies is required for its evaluation.

Example~\ref{ex:gain} show the gain computation using average precision on rank lists and the aggregation with Single-Mean and Single-Max.

\begin{example}
  \centering
  \caption{Gain}
  \label{ex:gain}

  \begin{subexample}{\linewidth}
    \centering
    \subcaption{Gain (One to one)}

    Here two rank lists are compared by evaluating the gain is average precision.

    \vspace{0.2cm}

    \begin{tabular}{l r}
      \toprule
      \textbf{Rank list} & \textbf{AP} \\
      \midrule
      Rank list A & $0.8$ \\
      Rank list B & $0.6$ \\
      \midrule
      \textbf{Gain} & \textbf{AP gain} \\
      A gain over B & $0.8 - 0.6 = +0.2$ \\
      \bottomrule
    \end{tabular}
  \end{subexample}

  \vspace{0.5cm}

  \begin{subexample}{\linewidth}
    \centering
    \subcaption{Gain (One to many)}

    Here one rank list is compared to two others by evaluating the gain is average precision using two aggregations strategies (Single-Mean, Single-Max).

    \vspace{0.2cm}

    \resizebox{\linewidth}{!}{
    \begin{tabular}{l r}
      \toprule
      \textbf{Rank list} & \textbf{AP} \\
      \midrule
      Rank list A & $0.8$ \\
      Rank list B & $0.6$ \\
      Rank list C & $0.9$ \\
      \midrule
      \textbf{Aggregation} & \textbf{AP} \\
      A-B (Single-Mean) & $(0.8 + 0.6) / 2 = 0.7$ \\
      A-B (Single-Max) & $\max(0.8, 0.6) = 0.8$ \\
      \midrule
      \textbf{Gain with aggregation} & \textbf{AP gain} \\
      C gain over A-B (Single-Mean) & $0.9 - 0.7 = +0.2$ \\
      C gain over A-B (Single-Max) & $0.9 - 0.8 = +0.1$ \\
      \bottomrule
    \end{tabular}
    }
  \end{subexample}

\end{example}
