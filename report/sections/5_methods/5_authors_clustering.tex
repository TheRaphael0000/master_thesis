\subsection{Authors Clustering \label{sec:authors_clustering}}

To find clusters of authors, a possible way is to use a hierarchical clustering algorithm on a rank list.
In a rank list, at each rank the link indicate if the two documents should belong to the same cluster in order of certainty.
The hardest task in this clustering scheme is to find where the list should be cut (also called \textit{threshold}).
The cut should be made such that the number of true links above the cut is maximized and the number of false links under the cut is minimized.
To find this cut, two approaches were explored : one using an unsupervised clustering evaluation technique which is totally unsupervised and another one using a linear model to learn to make this cut, but it requires a training dataset.

\subsubsection{Agglomerative clustering}

The scikit-learn package~\cite{sklearn} provide an implementation bottom-up implementation of the hierarchical clustering, which is usually called agglomerative clustering.
Using this approach, at the start of the algorithm, each document belong to a different cluster.
Clusters are merged based on the scores in the rank list, each step the algorithm merge clusters using the minimal score based on the linkage criteria.
Multiple linkage criteria are available : \textit{ward} (metric that aim to minimize the variance of the cluster merged), \textit{average-linkage} (use the average score of each link of the cluster merged), \textit{complete-linkage} (use the maximal score of the cluster merged), \textit{single-linkage} (use the minimal score of the cluster merged).
Ward linkage was discarded since the current implementation only allow euclidean distance for its computation.
The merging procedure can be stopped either of the two following criteria : When a certain number of cluster is reached or when the minimal score for the next merge is above a certain value (distance threshold).
In this study both stopping procedure are used, one in an unsupervised way and another one in a supervised way.

\subsubsection{Learning the threshold in a unsupervised way}

The idea is to run the agglomerative clustering on multiple time and stop at each number of clusters.
This produce $N$ possible clusterings, each of those can be evaluated using the silhouette score.
See definition~\ref{def:silhouette}.

\begin{definition}[Silhouette score~\cite{sklearn}]
  \label{def:silhouette}
  The silhouette score is a unsupervised clustering metric which evaluate a clustering result by measureing the cohesion and separation of the clusters.
  \begin{equation}
    \frac{b - a}{max(a, b)}
  \end{equation}
  \begin{equation*}
    \begin{split}
      a&: \text{intra-cluster distance}\\
      b&: \text{nearest-cluster distance}
    \end{split}
  \end{equation*}
  The value is ranged between -1 and 1, a large value indicate a good cohesion and good separation of the clusters (low intra-cluster distance, high nearest-cluster distance).
\end{definition}

When iterating over the number of clusters each times the median silhouette score for each cluster is computed.
When this score is below 0, the last positive score clusters is kept.
If the procedure does not reach a score below 0, the maximal is kept.
This procedure is called Iterative Positive Silhouette (IPS) and was proposed in~\cite{automated_unsupervised}.

\subsubsection{Learning the threshold in a supervised way}

To learn at which position in the rank list the cut should be, this second idea is to fit a linear model on samples created with the rank list.
To train the model, a sample is created for each link in a rank list.
The links labels are either \textit{true} when both document in the link are from the same author and \textit{false} otherwise.
The features used are : the log of the relative rank ($log(\frac{rank}{|L|})$ and the score of the link.
Since the training is only based on the relative rank and the score at each rank, the trained model is language independent and size independent.
But this model is metric dependent, since the score is not normalized.

In this study the model used is the logistic regression.
To find the cut on the test datasets, the fitted model is used on every links in the testing rank list, the number of \textit{true} labels give the rank at which the last link should be merged.
The links are labeled as \textit{true}, when the probability of being a true link is greater than $0.5$.
This model can be used on any other rank lists produced with the same distance metrics.
Using agglomerative clustering the maximal distance threshold is the score of the last link to merged.
