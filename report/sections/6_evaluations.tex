\section{Evaluation \label{sec:evaluation}}

This chapter explain the divers evaluation metrics used and experiments realized on the datasets to evaluate the proposed methods (ref. Section~\ref{sec:methods}).

\subsection{Rank list evaluation metrics}
\label{sec:rl_eval}

In order to know the quality of a rank list, the following metrics were used.
Definitions in this sections are an adapted version of the ones in~\cite{kocher_linking}.
The presented metrics are well know in the authorship verification and the information retrieval field.

\begin{definition}[Relevant link~\cite{kocher_linking}]
  A relevant link is a link in the relevant set.
  The relevant set is explain in Definition~\ref{def:relevant_set}.
  \begin{equation}
    relevant(l_i) =
    \begin{cases}
      1, & if\ l_i \in R \\
      0, & otherwise
    \end{cases}
  \end{equation}
\end{definition}

\begin{definition}[Precision@k~\cite{kocher_linking}]
  The precision@k is a function which take a positive integer k, with k < |L|
  \begin{equation}
    precision(k) = \frac{1}{k} \sum_{j=1}^{k} relevant(j)
  \end{equation}
\end{definition}

\begin{definition}[Average Precision (AP)]
  The mean over the precision@k each time a relevant link is retrieved.
  \begin{equation}
    AveragePrecision = \frac{1}{|R|} \sum_{j=1}^{|L|} precision(j) \cdot relevant(j)
  \end{equation}
  The average precision can be considered as an approximation of the area under the precision-recall curve.
\end{definition}

\begin{definition}[R-Precision~\cite{kocher_linking}]
  The R-Precision (RPrec) is the precision in the rank list at rank |R| (Precision@r).
  With R being the relevant set (Definition~\ref{def:relevant_set}).
  \begin{equation}
    RPrec = precision(|R|)
  \end{equation}
  The RPrec value is in the range $\left[0, 1\right]$.
  With 0 mean every links in the first $|R|$-ranks are not in the relevant set.
  And 1, every links in the first $|R|$-ranks are in the relevant set.
\end{definition}

\begin{definition}[High precision~\cite{kocher_linking}]
  The high precision (HPrec) represent the maximal rank j in the rank list such that the precision is still 100\%.
  \begin{equation}
    HPrec = \max\{i \in \mathbf{N} | precision(i) = 1\}
  \end{equation}
  This value is in the range $\left[0, |R|\right]$.
  $0$ means the first pair in the rank list is incorrect.
  $|R|$ means every true links are ranked in the top part of the rank list.
\end{definition}

\subsection{Importance of the text size in stylometry}

Previous studies have shown the importance of having documents of good quality and with at least 5000 tokens to have reliable results.
Skilled authors can easily change their style to imitate others for small texts, but it becomes more difficult for larger texts~\cite{savoy_stylo}.

For this study, an experiment on the St-Jean dataset was accomplished, in the order to show the importance of having large documents.
Figure~\ref{img:degradation} shows the three metrics used (Average precision, RPrec, HPrec) over the number of tokens.
The number of token was artificially modified by considering only the $n$ first tokens for each text, with $n$ ranging between 9000 and 250 with steps of 250 tokens.
Every metrics decrease over the text size, which means it becomes harder to determinate documents pairs with the same author as the text size decrease.

\begin{figure}
  \includegraphics[width=\linewidth]{img/degradation.png}
  \caption{St-Jean ranks list evaluation on AP, RPrec and HPrec over the text size. Rank list computed using 500 MFW and the zscored-normalized cosine distance}
  \label{img:degradation}
\end{figure}

PAN16 dataset is a difficult dataset due to its small size, thus extracting reliable features for each text to estimate each style is also a difficult task.
After multiple tests, the PAN @ CLEF 2016 dataset was not used further in this study due to its difficulty in finding reliable Stylometric clues.

\subsection{MFW Token and Lemma}

TODO

\subsection{MFW Letter n-grams}

TODO

\subsection{MFW First letters, last letters, word n-gram of tokens}

In this experiment, the goal is to create 3 types of text representations of word substrings and compare them.

\begin{enumerate}
  \item
  The first representation is the N-first letter of each word tokens which correspond generaly to the meaning of a word.
  This approach is closely related to the lemma approach.
  \item
  Extract also the N-last letter of each word tokens which in this case correspond to the role of the word in a sentence.
  This second approach is closely related to the POS approach.
  \item
  The \textit{word n-grams}, this special type of n-gram only consider N-grams within a word.
  This exclude every overlapping word n-grams.
  This decision is made for this experiment such that interwords style information can't be learned and have a fair comparaison of these 3 texts representations.
\end{enumerate}
The two first methods can be considered as a subsample of the n-grams approach, thus a simplified one (have smaller text representation).
The distance mesure used is the z-normalized cosine distance and the evaluation metric is the average precision.
The number of MFW variate for this experiment between 200 and 4000 with a step of 100.

Figure~\ref{fig:first_last_letters_ngrams_brunet} shows the results for the Brunet dataset and Figure~\ref{fig:first_last_letters_ngrams_oxquarry} for the oxquarry dataset.

In both dataset, the 3-last letters and 3-first letters gives a lower average precision and quickly converge to a equilibrium value at around 1500 MFW.
5-letters representations tend to produce better results than 3-letters and 4-letters representation as the MFW increase.
In the Oxquarry corpus, word 4-grams and word 5-grams give the better results with an average precision of ~95.0\%
For the brunet dataset, the 5-first letters can a good results compared to other text representations with small MFW vector size.

\begin{figure}
  \includegraphics[width=\linewidth]{img/first_last_letters_ngrams_brunet.png}
  \caption{Average precision over the MFW in the rank list generated using the z-score normalized cosine distance on the Brunet dataset.}
  \label{fig:first_last_letters_ngrams_brunet}
\end{figure}
\begin{figure}
  \includegraphics[width=\linewidth]{img/first_last_letters_ngrams_oxquarry.png}
  \caption{Average precision over the MFW in the rank list generated using the z-score normalized cosine distance on the Oxquarry dataset.}
  \label{fig:first_last_letters_ngrams_oxquarry}
\end{figure}

\subsection{MFW POS n-grams}

For this experiment short combination of POS are used to detect the style of the author.
The St-Jean dataset have for every word a POS tag, by using these POS tags and combining them using Definition~\ref{def:n-grams} to create n-grams/w-shingling, a new text representation is obtained.
Using this representation the N-MFW are computed and the rank list containing every document pairs can be computed.

In this experiment, only 2-grams, 3-grams, 4-grams and the combination of the 2-grams and 3-grams denoted: (2, 3)-grams is used.
The distance metric used is the smoothed z-score normalized cosine distance.
For this representation no clear MFW (most frequent word, in this case POS n-grams are considered as words) vector size is advised, the size used is between 200 and 2000 with a step of 100.
Figure~\ref{fig:pos_ngrams} show the average precision on the rank list produced by using POS n-grams over the number of MFW.

The two following informations can be intuitively observed on this plot:
\begin{itemize}
  \item
  A more complex POS n-gram require more MFW to archive its maximal effectiveness.
  In the St-Jean corpus 26 POS are used to describe every words in the corpus.
  Which correspond to $26^2 = 676$ possible unique POS 2-grams, to $26^3 = 17'576$ POS 3-grams and $26^4 = 456'976$ POS 4-grams.
  \item
  Like other methods, if the MFW ceiling is too high, an overfitting to less important words is possible, thus reducing the average precision.
  In Figure~\ref{fig:pos_ngrams} the POS 2-grams clearly have a drop in average precision after \~250-MFW.
\end{itemize}

\begin{figure}
  \includegraphics[width=\linewidth]{img/pos_ngrams.png}
  \caption{Average precision over the MFW in the rank list generated using the z-score normalized cosine distance.}
  \label{fig:pos_ngrams}
\end{figure}

\subsection{Compression based distances}

This experiment try to compare the three proposed compression algorithm (GZip, BZip2, LZMA) for the compression based distance ranking.
For each algorithm for each document the size after compression is computed, as well as the concatenation of every document pairs.
Using these sizes and the NCD and CBC distance metrics (ref. Section~\ref{sec:compression_based_distances}, the rank list is evaluated.
This experiment was run on the three datasets three times to have a better approximation of the run time.
The results in terms of efficiency of the resulting rank list are shown in Figures~\ref{fig:compression_evaluation_oxquarry}~/~\ref{fig:compression_evaluation_brunet}~\ref{fig:compression_evaluation_st_jean}.
The average time of the 3 runs are in Tables~\ref{tab:compression_evaluation_oxquarry}~/~\ref{tab:compression_evaluation_brunet}~/~\ref{tab:compression_evaluation_st_jean}.

GZip seem to be giving the worse results on every dataset with an average AP of \~-0.20 in compared to LZMA and BZip2.
LZMA give the best results on every dataset tested, but BZip2 have close results.
The cosine-based compression distance (CBC) tend to give better results over the normalized compression distance (NCD).
In terms of time complexity BZip2 is the fastest algorithm of the 3 proposed, LZMA is ~5-6 times slower than BZip2, and GZip slower than BZip2 by around ~5-20\%.
No significant time differences was recorded between the NCD and CBC distance measures, since the greater complexity is in the compression algorithm.

\begin{figure}
  \includegraphics[width=\linewidth]{img/compression_evaluation_oxquarry.png}
  \caption{Evaluation of the different compression algorithm and distance metrics on Oxquarry}
  \label{fig:compression_evaluation_oxquarry}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{img/compression_evaluation_brunet.png}
  \caption{Evaluation of the different compression algorithm and distance metrics on Brunet}
  \label{fig:compression_evaluation_brunet}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{img/compression_evaluation_st_jean.png}
  \caption{Evaluation of the different compression algorithm and distance metrics on St-Jean}
  \label{fig:compression_evaluation_st_jean}
\end{figure}

\begin{table}
  \centering
  \caption{Run time for the rank list computation with the different compression algorithm and distance metrics on Oxquarry}
  \label{tab:compression_evaluation_oxquarry}
  \begin{tabular}{l c c c}
    \toprule
         & Bzip2 & GZip  & LZMA \\
    \midrule
    NCD  & 12.7s & 15.0s & 69.0s \\
    CBC  & 12.7s & 14.9s & 68.8s \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}
  \centering
  \caption{Run time for the rank list computation with the different compression algorithm and distance metrics on Brunet}
  \label{tab:compression_evaluation_brunet}
  \begin{tabular}{l c c c}
    \toprule
         & Bzip2 & GZip & LZMA \\
    \midrule
    NCD  & 8.4s  & 8.8s & 46.6s \\
    CBC  & 8.4s  & 8.9s & 46.8s \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}
  \centering
  \caption{Run time for the rank list computation with the different compression algorithm and distance metrics on St-Jean}
  \label{tab:compression_evaluation_st_jean}
  \begin{tabular}{l c c c}
    \toprule
         & Bzip2  & GZip   & LZMA \\
    \midrule
    NCD  & 198.9s & 211.3s & 1046.3s \\
    CBC  & 198.4s & 214.5s & 1052.0s \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Frequent errors}

This section try to understand the errors in our system, in this case the false links (document pairs with different authors) highly ranked on different rank list.
The rank list quality is highly based on the feature vector created using the N-MFW, having a good understanding of this vector give good indications of the strength of the system.

To find recurrent errors in our system we use 5 different distance metrics on the relative frequency of the 500-MFW using the St-Jean dataset.
This generates 5 rank lists.
The average precision for these rank list is always greater than 0.7.
With this 5 rank lists only the top 10 false links are kept to be analyzed.
Frequent errors are links that often appear in this top 10.
Figures~\ref{fig:mfw_vector_error_0}~/~\ref{fig:mfw_vector_error_1} show two pairs (Zola 49 / Flaubert 63 and Maupassant 10 / Flaubert 52) that appear 4 time out of the 5 rank lists in the top 10 false links, thus frequent error.

\begin{figure}
  \includegraphics[width=\linewidth]{img/mfw_vector_error_0.png}
  \caption{First example of 500-MFW relative frequency vector for the two documents in a reccurant (4 rank lists out of 5) false link in the top 10 false links}
  \label{fig:mfw_vector_error_0}
\end{figure}
\begin{figure}
  \includegraphics[width=\linewidth]{img/mfw_vector_error_1.png}
  \caption{Second example of 500-MFW relative frequency vector for the two documents in a reccurant (4 rank lists out of 5) false link in the top 10 false links}
  \label{fig:mfw_vector_error_1}
\end{figure}

To be able to understand more easily this vector, the values have been sorted by relative frequencies.
When a large proportion of the vectors overlap it indicates a high similarity between the vectors.
In this case most of the surface overlap, so the distance function will give a low value, and rank this vector high in the list.
Both document style are close when their feature vector are closely related.
We can not clearly determinate that these texts are from a different author using only this type of representation.
These two vectors pair can be visually compared to the most similar link (ranked 1 using Manhattan distance) in Figure~\ref{fig:mfw_vector_first_rl} (Stael 157 / Steal 183) or the HPrec-th (last continous correct pair from the top of the list) in Figure~\ref{fig:mfw_vector_first_last_rl} (Maupassant 10 / Maupassant 67), both of these links show a large proportion of overlapping surface.
A counter example would be the least similar link (ranked last using Manhattan distance) which represent a negatively correlated document pair, Figure~\ref{fig:mfw_vector_last_rl} showcase this link.
As expected, most of this figure surface is non-overlapping.

\begin{figure}
  \includegraphics[width=\linewidth]{img/mfw_vector_first_rl.png}
  \caption{500-MFW relative frequency for the two documents ranked first in the rank list}
  \label{fig:mfw_vector_first_rl}
\end{figure}
\begin{figure}
  \includegraphics[width=\linewidth]{img/mfw_vector_first_last_rl.png}
  \caption{500-MFW relative frequency for the two documents ranked HPrec-th in the rank list}
  \label{fig:mfw_vector_first_last_rl}
\end{figure}
\begin{figure}
  \includegraphics[width=\linewidth]{img/mfw_vector_last_rl.png}
  \caption{500-MFW relative frequency for the two documents ranked last in the ranked list}
  \label{fig:mfw_vector_last_rl}
\end{figure}

\subsection{Publication date differences analyis}

When dealing with false links ranked high in the rank list, as the previous experiment showed, some excerpt use similar words.
These shared words might be related to the era the book was written in.
The following experiment try to investigate on this.

In the St-Jean dataset publication paper, the publication dates of each excepts are available~\cite{st_jean}.
First the publication date distribution of the dataset must be understood.
Figure~\ref{fig:dates_distribution} show the distribution of the publication date in the St-Jean dataset.
Figure~\ref{fig:dates_differences_all} show the date difference distribution for each pair of documents, we can see that the average date difference in the dataset is 28.24 years with a standard deviation of 20.73 years.
Since this dataset contain multiple excerpt from the same book, we might consider only the links of different authors (false links) Figure~\ref{fig:dates_differences_false} shows such distribution.
As expected the mean increased to 29.04 years since there are fewer links with small date difference.
Same authors links (true links) are displayed in Figure~\ref{fig:dates_differences_r_true}, they confirm the previous statement that most of the same authors links have a low date difference with a mean of 5.11 years.

\begin{figure}
  \includegraphics[width=\linewidth]{img/dates_distribution.png}
  \caption{Date distribution in the St-Jean dataset.}
  \label{fig:dates_distribution}
\end{figure}
\begin{figure}
  \includegraphics[width=\linewidth]{img/dates_differences_all.png}
  \caption{Pairwise date difference denstiy in St-Jean for all the excerpt.}
  \label{fig:dates_differences_all}
\end{figure}
\begin{figure}
  \includegraphics[width=\linewidth]{img/dates_differences_false.png}
  \caption{Pairwise date difference denstiy in St-Jean for all the excerpt with different authors (false links).}
  \label{fig:dates_differences_false}
\end{figure}
\begin{figure}
  \includegraphics[width=\linewidth]{img/dates_differences_r_true.png}
  \caption{Pairwise date difference denstiy in St-Jean for all the excerpt with similar authors (true links) also correspond to the top-r true links.}
  \label{fig:dates_differences_r_true}
\end{figure}

Figure~\ref{fig:dates_differences_r_false} show the date difference density on the top-r false links (670 in case of St-Jean) on a rank list with 80\% average precision.
Two interesting information can be extracted here.
First the mean is lower by 8.22 years (29.04 - 20.82) comparing to the false links distribution which clearly indicate the importance of publication date in the ranking of the documents.
Secondly we can observe a drop at 35 years of date difference, which indicate that links in the interval $\left[0-35\right]$ are harder to discriminate than links outside this interval.
This 35 years interval can be related to the generation factor, the age of woman giving birth is around 25-34 in France \cite{generations}.
Each new generation use its own vocabulary and can be harder to discriminate author of text belonging to the same generation, in the other hand having different vocabulary can indicate a different time period and is often used to detect document forgery \cite{savoy_stylo}.

\begin{figure}
  \includegraphics[width=\linewidth]{img/dates_differences_r_false.png}
  \caption{Pairwise date difference denstiy in St-Jean for top-r false links using a rank list with 80\% average precision.}
  \label{fig:dates_differences_r_false}
\end{figure}

\subsection{Rank lists fusion evaluation}

\begin{table}[h]
  \caption{S-Curve/Equal/Single max,  * : Binomial test p-value < 5\%}
  \centering
  \label{}
  \begin{tabular}{l c c c}
    \toprule
    Metric & St-Jean  & Brunet & Oxquarry \\ \midrule
    AP     & 161/0/334* & 208/0/287* & 1/0/494*\\
    RPrec  & 169/9/317* & *378/75/42 & 0/2/493*\\
    HPrec  & *287/1/207 & 49/87/359* & 32/11/452*\\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[h]
  \caption{Z-Score/Equal/Single max,  * : Binomial test p-value < 5\%}
  \centering
  \label{}
  \begin{tabular}{l c c c}
    \toprule
    Metric& St-Jean  & Brunet & Oxquarry \\ \midrule
    AP    & *284/0/211  & *285/0/210 & 1/0/494*\\
    RPrec & *312/10/173 & *361/94/40 & 0/0/495*\\
    HPrec & *289/0/206  & 35/63/397* & 79/13/403*\\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[h]
  \caption{S-Curve/Equal/Z-Score,  * : Binomial test p-value < 5\%}
  \centering
  \label{}
  \begin{tabular}{l c c c}
    \toprule
    Metric & St-Jean & Brunet & Oxquarry \\ \midrule
    AP     & 7/0/488*   & 42/0/453*    & *270/0/225\\
    RPrec  & 4/3/488*   & 124/241/130  & 51/96/348*\\
    HPrec  & 57/18/420* & *188/202/105 & 70/34/391*\\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}
  \includegraphics[width=\linewidth]{img/fusion_st_jean.png}
  \caption{}
  \label{fig:fusion_st_jean}
\end{figure}
\begin{figure}
  \includegraphics[width=\linewidth]{img/fusion_brunet.png}
  \caption{}
  \label{fig:fusion_brunet}
\end{figure}
\begin{figure}
  \includegraphics[width=\linewidth]{img/fusion_oxquarry.png}
  \caption{}
  \label{fig:fusion_oxquarry}
\end{figure}

\subsection{Clustering evaluation metrics}

\begin{definition}[Correctness~\cite{bcubed}]
  Let L(e) and C(e) be the category and the cluster of an element e.
  The correctness has a value of one if the two elements are in the both in the same cluster and has the same category OR both in a different cluster and a different category. ($A \Longleftrightarrow B \equiv (A \land B) \lor (\neg A \land \neg B)$)
  \begin{gather*}
    Correctness(e, e') = \\
    \begin{cases}
      1, & if (L(e) = L(e')) \Longleftrightarrow (C(e) = C(e'))\\
      0, & otherwise
    \end{cases}
  \end{gather*}
\end{definition}

\begin{definition}[Precision $BCubed$~\cite{bcubed}]
  The average of correctness for all elements on the average of all element such that their cluster is the same.
  \begin{equation}
    BCubed_{precision} = \text{Avg}_{e}[\text{Avg}_{e' C(e)=C(e')}[Correctness(e, e')]]
  \end{equation}
\end{definition}

\begin{definition}[Recall $BCubed$~\cite{bcubed}]
  The average of correctness for all elements on the average of all element such that their category is the same.
  \begin{equation}
    BCubed_{recall} = \text{Avg}_{e}[\text{Avg}_{e' L(e)=L(e')}[Correctness(e, e')]]
  \end{equation}
\end{definition}

\begin{definition}[$BCubed F_1$ Score~\cite{bcubed}]
  The harmonic mean between the $BCubed_{precision}$ and $BCubed_{recall}$
  \begin{equation}
    BCubed_{F_1} =
    2 \cdot \frac{BCubed_{precision} \cdot BCubed_{recall}}
    {BCubed_{precision} + BCubed_{recall}}
  \end{equation}
\end{definition}
