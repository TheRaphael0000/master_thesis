\section{Evaluation \label{sec:evaluation}}

This chapter explain the divers evaluation metrics used and experiments realized on the datasets to evaluate the proposed methods (ref. Section~\ref{sec:methods}).

\subsection{Rank list evaluation metrics}
\label{sec:rl_eval}

In order to know the quality of a rank list, the following metrics were used.
Definitions in this sections are an adapted version of the ones in~\cite{kocher_linking}.
The presented metrics are well know in the authorship verification and the information retrieval field.

\begin{definition}[Relevant link~\cite{kocher_linking}]
  A relevant link is a link in the relevant set.
  The relevant set is explain in Definition~\ref{def:relevant_set}.
  \begin{equation}
    relevant(l_i) =
    \begin{cases}
      1, & if\ l_i \in R \\
      0, & otherwise
    \end{cases}
  \end{equation}
\end{definition}

\begin{definition}[Precision@k~\cite{kocher_linking}]
  The precision@k is a function which take a positive integer k, with k < |L|
  \begin{equation}
    precision(k) = \frac{1}{k} \sum_{j=1}^{k} relevant(j)
  \end{equation}
\end{definition}

\begin{definition}[Average Precision (AP)]
  The mean over the precision@k each time a relevant link is retrieved.
  \begin{equation}
    AveragePrecision = \frac{1}{|R|} \sum_{j=1}^{|L|} precision(j) \cdot relevant(j)
  \end{equation}
  The average precision can be considered as an approximation of the area under the precision-recall curve.
\end{definition}

\begin{definition}[R-Precision~\cite{kocher_linking}]
  The R-Precision (RPrec) is the precision in the rank list at rank |R| (Precision@r).
  With R being the relevant set (Definition~\ref{def:relevant_set}).
  \begin{equation}
    RPrec = precision(|R|)
  \end{equation}
  The RPrec value is in the range $\left[0, 1\right]$.
  With 0 mean every links in the first $|R|$-ranks are not in the relevant set.
  And 1, every links in the first $|R|$-ranks are in the relevant set.
\end{definition}

\begin{definition}[High precision~\cite{kocher_linking}]
  The high precision (HPrec) represent the maximal rank j in the rank list such that the precision is still 100\%.
  \begin{equation}
    HPrec = \max\{i \in \mathbf{N} | precision(i) = 1\}
  \end{equation}
  This value is in the range $\left[0, |R|\right]$.
  $0$ means the first pair in the rank list is incorrect.
  $|R|$ means every true links are ranked in the top part of the rank list.
\end{definition}

\subsection{Importance of the text size in stylometry}

Previous studies have shown the importance of having documents of good quality and with at least 5000 tokens to have reliable results.
Skilled authors can easily change their style to imitate others for small texts, but it becomes more difficult for larger texts~\cite{savoy_stylo}.

For this study, an experiment on the St-Jean dataset was accomplished, in the order to show the importance of having large documents.
Figure~\ref{img:degradation} shows the three metrics used (Average precision, RPrec, HPrec) over the number of tokens.
The number of token was artificially modified by considering only the $n$ first tokens for each text, with $n$ ranging between 9000 and 250 with steps of 250 tokens.
Every metrics decrease over the text size, which means it becomes harder to determinate documents pairs with the same author as the text size decrease.

\begin{figure}
  \includegraphics[width=\linewidth]{img/degradation.png}
  \caption{St-Jean ranks list evaluation on AP, RPrec and HPrec over the text size. Rank list computed using 500 MFW and the zscored-normalized cosine distance}
  \label{img:degradation}
\end{figure}

PAN16 dataset is a difficult dataset due to its small size, thus extracting reliable features for each text to estimate each style is also a difficult task.
After multiple tests, the PAN @ CLEF 2016 dataset was not used further in this study due to its difficulty in finding reliable Stylometric clues.

\subsection{POS n-grams}

For this experiment short combination of POS are used to detect the style of the author.
The St-Jean dataset have for every word a POS tag, by using these POS tags and combining them using Definition~\ref{def:n-grams} to create n-grams/w-shingling, a new text representation is obtained.
Using this representation the N-MFW are computed and the rank list containing every document pairs can be computed.

In this experiment, only 2-grams, 3-grams, 4-grams and the combination of the 2-grams and 3-grams denoted: (2, 3)-grams is used.
For this representation no clear MFW (most frequent word, in this case POS n-grams are considered as words) vector size is advised, the size used is between 200 and 2000 with a step of 100.
Figure~\ref{fig:pos_ngrams} show the average precision on the rank list produced by using POS n-grams over the number of MFW.

The two following informations can be intuitively observed on this plot:

\begin{itemize}
  \item
  A more complex POS n-gram require more MFW to archive its maximal effectiveness.
  In the St-Jean corpus 26 POS are used to describe every words in the corpus.
  Which correspond to $26^2 = 676$ possible unique POS 2-grams, to $26^3 = 17'576$ POS 3-grams and $26^4 = 456'976$.
  \item
  Like other methods, if the MFW ceiling is too high, an overfitting to less important words is possible, thus reducing the average precision.
  In Figure~\ref{fig:pos_ngrams} the POS 2-grams clearly have a drop in average precision after \~250-MFW.
\end{itemize}

\begin{figure}
  \includegraphics[width=\linewidth]{img/pos_ngrams.png}
  \caption{Average precision over the MFW in the rank list generated using the z-score normalized cosine distance.}
  \label{fig:pos_ngrams}
\end{figure}

\subsection{First letters, last letters, word n-gram of tokens}

In this experiment, the goal is to extract 3 types of word substrings and compare them.

\begin{enumerate}
  \item
  The N first letter of each word tokens which correspond generaly to the meaning of a word.
  This approach is closely related to the lemma approach.
  \item
  Extract also the N last letter of each word tokens which in this case correspond to the role of the word in a sentence.
  This second approach is closely related to the POS approach.
  \item
  And the \texit{word n-grams}, this special type of n-gram only consider N-grams within a word.
  For example
\end{enumerate}
The two first methods can be considered as a subsample of the n-grams approach, thus a simplified one.

Figure~\ref{fig:first_last_letters_ngrams_brunet} shows the results for the Brunet dataset and Figure~\ref{fig:first_last_letters_ngrams_oxquarry} for the oxquarry dataset.
The distance mesure used is the z-normalized cosine distance and the evaluation metric is the average precision.

\begin{figure}
  \includegraphics[width=\linewidth]{img/first_last_letters_ngrams_brunet.png}
  \caption{Average precision over the MFW in the rank list generated using the z-score normalized cosine distance on the Brunet dataset.}
  \label{fig:first_last_letters_ngrams_brunet}
\end{figure}
\begin{figure}
  \includegraphics[width=\linewidth]{img/first_last_letters_ngrams_oxquarry.png}
  \caption{Average precision over the MFW in the rank list generated using the z-score normalized cosine distance on the Oxquarry dataset.}
  \label{fig:first_last_letters_ngrams_oxquarry}
\end{figure}

\subsubsection{Frequent errors}

This section, try to understand the errors in our system, in this case the false links (document pairs with different authors) highly ranked on different rank list.
The rank list quality is highly based on the feature vector created using the N-MFW, having a good understanding of this vector give good indications of the strength of the system.

To find reccurant errors in our system we use 5 different distance metrics on the relative frequency of the 500-MFW using the St-Jean dataset.
This generate 5 rank lists.
The average precision for these rank list is always greater than 0.7.
With this 5 rank lists only the top 10 false links are kept to be analyzed.
Frequent errors are link that appear often in this top 10.
Figures~\ref{fig:mfw_vector_error_0}~/~\ref{fig:mfw_vector_error_1} show two pairs (Zola 49 / Flaubert 63 and Maupassant 10 / Flaubert 52) that appear 4 time out of the 5 rank lists in the top 10 false links, thus frequent error.

\begin{figure}
  \includegraphics[width=\linewidth]{img/mfw_vector_error_0.png}
  \caption{First example of 500-MFW relative frequency vector for the two documents in a reccurant (4 rank lists out of 5) false link in the top 10 false links}
  \label{fig:mfw_vector_error_0}
\end{figure}
\begin{figure}
  \includegraphics[width=\linewidth]{img/mfw_vector_error_1.png}
  \caption{Second example of 500-MFW relative frequency vector for the two documents in a reccurant (4 rank lists out of 5) false link in the top 10 false links}
  \label{fig:mfw_vector_error_1}
\end{figure}

To be able to understand more easily this vector, the values have been sorted by relative frequencies.
When a large propostion of the vectors overlap it indicates a high similarities between the vectors.
In this case most of the surface overlap so the distance function will give a low value, and rank this vector high in the list.
Both document style are close when their feature vector are closely related.
We can't clearly determinate that these texts are from different author using only this type of representation.
These two vectors pair can be vizually compared to the most similar link (ranked 1 using manhattan distance) in Figure~\ref{fig:mfw_vector_first_rl} (Stael 157 / Steal 183) or the HPrec-th (last continous correct pair from the top of the list) in Figure~\ref{fig:mfw_vector_first_last_rl} (Maupassant 10 / Maupassant 67), both of these links show a large propotion of overlapping surface.
A counter example would be the least similar link (ranked last using manhattan distance) which represent a negatively correlated document pair, Figure~\ref{fig:mfw_vector_last_rl} showcase this link.
As expected, most of this figure surface is non-overlapping.

\begin{figure}
  \includegraphics[width=\linewidth]{img/mfw_vector_first_rl.png}
  \caption{500-MFW relative frequency for the two documents ranked first in the rank list}
  \label{fig:mfw_vector_first_rl}
\end{figure}
\begin{figure}
  \includegraphics[width=\linewidth]{img/mfw_vector_first_last_rl.png}
  \caption{500-MFW relative frequency for the two documents ranked HPrec-th in the rank list}
  \label{fig:mfw_vector_first_last_rl}
\end{figure}
\begin{figure}
  \includegraphics[width=\linewidth]{img/mfw_vector_last_rl.png}
  \caption{500-MFW relative frequency for the two documents ranked last in the ranked list}
  \label{fig:mfw_vector_last_rl}
\end{figure}

\subsubsection{Publication date differences analyis}

When dealing with false links ranked high in the rank list, as the previous experiment showed, some excerpt use similar words.
These shared words might be related to the era the book was written in.
The following experiment try to investigate on this.

In the St-Jean dataset publication paper, the publication dates of each excepts are available~\cite{st_jean}.
First the publication date distribution of dataset must be understood.
Figure~\ref{fig:dates_distribution} show the distribution of the publication date in the St-Jean dataset.
Figure~\ref{fig:dates_differences_all} show the date difference distribution for each pairs of document, we can see that the average date difference in the dataset is 28.24 years with a standard deviation of 20.73 years.
Since this dataset contain multiple excerpt from the same book, we might consider only the links of differents authors (false links) Figure~\ref{fig:dates_differences_false} show such distribution.
As expected the mean increased to 29.04 years since there are less links with small date difference.
Same authors links (true links) are displayed in Figure~\ref{fig:dates_differences_r_true}, they confirm the previous statement that most of the same authors links have a low date difference with a mean at 5.11 years.

\begin{figure}
  \includegraphics[width=\linewidth]{img/dates_distribution.png}
  \caption{Date distribution in the St-Jean dataset.}
  \label{fig:dates_distribution}
\end{figure}
\begin{figure}
  \includegraphics[width=\linewidth]{img/dates_differences_all.png}
  \caption{Pairwise date difference denstiy in St-Jean for all the excerpt.}
  \label{fig:dates_differences_all}
\end{figure}
\begin{figure}
  \includegraphics[width=\linewidth]{img/dates_differences_false.png}
  \caption{Pairwise date difference denstiy in St-Jean for all the excerpt with different authors (false links).}
  \label{fig:dates_differences_false}
\end{figure}
\begin{figure}
  \includegraphics[width=\linewidth]{img/dates_differences_r_true.png}
  \caption{Pairwise date difference denstiy in St-Jean for all the excerpt with similar authors (true links) also correspond to the top-r true links.}
  \label{fig:dates_differences_r_true}
\end{figure}

Figure~\ref{fig:dates_differences_r_false} show the date difference density on the top-r false links (670 in case of St-Jean) on a rank list with 80\% average precision.
Two interesting informations can be extracted here.
First the mean is lower by 8.22 years (29.04 - 20.82) comparing to the false links distribution which clearly indicate a importance of publication date in the ranking of the documents.
Secondly we can obseve a drop at 35 years of date difference, which indicate that links in the interval $\left[0-35\right]$ are harder to discriminate than links outside this interval.
This 35 years interval can be related to the generation factor, the age of woman giving birth is around 25-34 in France \cite{generations}.
Each new generation use its own vocabulary and can be harder to discriminate author of text beloning to the same generation, in the other hand having differant vocabulary can indicate a differant time periode and is often use to detect document forgery \cite{savoy_stylo}.

\begin{figure}
  \includegraphics[width=\linewidth]{img/dates_differences_r_false.png}
  \caption{Pairwise date difference denstiy in St-Jean for top-r false links using a rank list with 80\% average precision.}
  \label{fig:dates_differences_r_false}
\end{figure}


\subsubsection{Rank lists fusion evaluation}

\begin{table}[h]
  \caption{S-Curve/Equal/Single max,  * : Binomial test p-value < 5\%}
  \centering
  \label{}
  \begin{tabular}{l c c c}
    \toprule
    Metric & St-Jean  & Brunet & Oxquarry \\ \midrule
    AP     & 161/0/334* & 208/0/287* & 1/0/494*\\
    RPrec  & 169/9/317* & *378/75/42 & 0/2/493*\\
    HPrec  & *287/1/207 & 49/87/359* & 32/11/452*\\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[h]
  \caption{Z-Score/Equal/Single max,  * : Binomial test p-value < 5\%}
  \centering
  \label{}
  \begin{tabular}{l c c c}
    \toprule
    Metric& St-Jean  & Brunet & Oxquarry \\ \midrule
    AP    & *284/0/211  & *285/0/210 & 1/0/494*\\
    RPrec & *312/10/173 & *361/94/40 & 0/0/495*\\
    HPrec & *289/0/206  & 35/63/397* & 79/13/403*\\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[h]
  \caption{S-Curve/Equal/Z-Score,  * : Binomial test p-value < 5\%}
  \centering
  \label{}
  \begin{tabular}{l c c c}
    \toprule
    Metric & St-Jean & Brunet & Oxquarry \\ \midrule
    AP     & 7/0/488*   & 42/0/453*    & *270/0/225\\
    RPrec  & 4/3/488*   & 124/241/130  & 51/96/348*\\
    HPrec  & 57/18/420* & *188/202/105 & 70/34/391*\\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}
  \includegraphics[width=\linewidth]{img/fusion_st_jean.png}
  \caption{}
  \label{fig:fusion_st_jean}
\end{figure}
\begin{figure}
  \includegraphics[width=\linewidth]{img/fusion_brunet.png}
  \caption{}
  \label{fig:fusion_brunet}
\end{figure}
\begin{figure}
  \includegraphics[width=\linewidth]{img/fusion_oxquarry.png}
  \caption{}
  \label{fig:fusion_oxquarry}
\end{figure}

\subsection{Clustering evaluation metrics}

\begin{definition}[Correctness~\cite{bcubed}]
  Let L(e) and C(e) be the category and the cluster of an element e.
  The correctness has a value of one if the two elements are in the both in the same cluster and has the same category OR both in a different cluster and a different category. ($A \Longleftrightarrow B \equiv (A \land B) \lor (\neg A \land \neg B)$)
  \begin{gather*}
    Correctness(e, e') = \\
    \begin{cases}
      1, & if (L(e) = L(e')) \Longleftrightarrow (C(e) = C(e'))\\
      0, & otherwise
    \end{cases}
  \end{gather*}
\end{definition}

\begin{definition}[Precision $BCubed$~\cite{bcubed}]
  The average of correctness for all elements on the average of all element such that their cluster is the same.
  \begin{equation}
    BCubed_{precision} = \text{Avg}_{e}[\text{Avg}_{e' C(e)=C(e')}[Correctness(e, e')]]
  \end{equation}
\end{definition}

\begin{definition}[Recall $BCubed$~\cite{bcubed}]
  The average of correctness for all elements on the average of all element such that their category is the same.
  \begin{equation}
    BCubed_{recall} = \text{Avg}_{e}[\text{Avg}_{e' L(e)=L(e')}[Correctness(e, e')]]
  \end{equation}
\end{definition}

\begin{definition}[$BCubed F_1$ Score~\cite{bcubed}]
  The harmonic mean between the $BCubed_{precision}$ and $BCubed_{recall}$
  \begin{equation}
    BCubed_{F_1} =
    2 \cdot \frac{BCubed_{precision} \cdot BCubed_{recall}}
    {BCubed_{precision} + BCubed_{recall}}
  \end{equation}
\end{definition}
