\section{Compression-based Distances \label{sec:compression_based_distances}}

This section covers another method to compute distances between documents based on file compression.

\subsection{Method}

The main idea is to first compress two documents A, B then compress the concatenation of A and B, denoted AB.

Using the sizes after compression of A, B, AB and a compression distance measure, it is possible to compute a distance between A and B.

For the compression, a lossless compression algorithm is used.
The Lempel-Ziv family (GZIP), the block sorting family (BZip2) and the statistical family (PPM) have been experimented in Oliveira and al. \cite{comparing_compression} and show good results.

This technique is based on the fact that lossless compression algorithms tries to lower the Shannon entropy of a document.
When compressing a document with a large Shannon entropy, the compressed document should have a larger size after compression than a document with a small Shannon entropy.
When concatenating two documents that share many terms, the entropy of the concatenated document should be lower than if the two documents present distinct vocabulary.

This approach has the benefit to produce rank lists in a nearly parameterless manner, only a distance metric and compression algorithm are needed.
The main drawback with this technique is the fact that the results can not be entirely explained.

In this study the lossless compression algorithm used are: GZip, BZip2, LZMA.
The implementations for these compression algorithms are the ones in the Python standard library, the programming language used for this study \cite{python_standard_library}.

Each compression algorithm can be tweaked with different parameters, the default settings are used except for the compression level (trade-off compression time and compression size).
We set the compression level to the maximal setting.
This allows to ensure that the produced file will have the lowest possible Shannon entropy reachable with this algorithm.
Thus providing the best possible approximation of distance when used in conjunction with the compression distances.

Definitions~\ref{def:compress1},~\ref{def:compress2} and \ref{def:compress3} are compression distance measure found in the literature  \cite{comparing_compression} \cite{savoy_stylo}.

\begin{definition}[Conditional complexity of compression~\cite{comparing_compression}~\cite{savoy_stylo}\label{def:compress1}]
  The conditional complexity of compression of two documents A and B is computed as follows:
  \begin{gather*}
    CCC(A, B) = C(AB) - C(A)
  \end{gather*}
  C(AB) is the size after compression of the concatenation of A and B

  C(A) the size after compression of A.

  This metrics is not easy to use since the order of magnitude is not bounded and can depend a lot on the text sizes.
  The next ones try to mitigate this problem.
\end{definition}

\begin{definition}[Normalized compression distance~\cite{comparing_compression}~\cite{savoy_stylo}\label{def:compress2}]
  The normalized compression distance of two documents A and B is computed as follows:
  \begin{gather*}
    NCD(A, B) = \frac{C(AB) - \min(C(A), C(B))}{\max(C(A), C(B))}
  \end{gather*}
  C(AB) is the size after compression of the concatenation of A and B

  C(A) the size after compression of A

  C(B) the size after compression of B.

  This metric gives a value in the range $\left[0, 1+\epsilon\right]$, with $\epsilon$ being a small positive value created by the imperfection of compression algorithms.
\end{definition}

\begin{definition}[Compression-based cosine~\cite{comparing_compression}~\cite{savoy_stylo}\label{def:compress3}]
  The compression-based cosine of two documents A and B is computed as follows:
  \begin{gather*}
    CBC(A, B) = 1 - \frac{C(A) + C(B) - C(AB)}{\sqrt{C(A) \cdot C(B)}}
  \end{gather*}
  C(AB) is the size after compression of the concatenation of A and B

  C(A) the size after compression of A

  C(B) the size after compression of B.

  This metric has the bounds as the cosine distance (Definition~\ref{def:cosine_dist}).
\end{definition}

\subsection{Evaluation}

An experiment was conducted to try to compare the three proposed compression algorithm (GZip, BZip2, LZMA) for the compression based distance ranking.

For each document, the size after compression is computed with each algorithm.
We also compute the size after compression of the concatenation of each document pair.

Using these sizes and the NCD or CBC distance metrics (ref. Section~\ref{sec:compression_based_distances}), the rank list are computed and evaluated.
The results in terms of efficiency of the resulting rank list are shown in Table~\ref{tab:compression_evaluation_results}.

The experiment is run on the three corpora three times to have a better approximation of the run time.
The average time of the three runs are in Table~\ref{tab:compression_evaluation_times}.
The CPU used for the experiment is an \textit{Intel(R) Core(TM) i7-5820K CPU @ 3.30GHz} .

LZMA gives the best results on every corpus tested (Mean AP: 0.76 with NCD and 0.78 with CBC).
BZip2 have results close to LZMA (LZMA have an AP $\sim 2.5\%$ better).
GZip give the worse results on every corpus.
It obtains an AP $\sim 30\%$ worse than LZMA.

The Cosine-based compression distance (CBC) tend to give slightly better results over the normalized compression distance (NCD).
The AP is $\sim 1\%$ better with the CBC.

In terms of time complexity, BZip2 is the fastest algorithm of the 3 proposed.
GZip is slower than BZip2 by around $\sim 7\%$.
LZMA is $\sim 5-6$ times slower than BZip2.

No significant time differences are recorded between the NCD and CBC distance measures.
This is explained by the fact that, the greatest complexity reside in the compression algorithm.

Previous study show that the distance measure choice was more impactful than the choice of the compression algorithm in the regarding the quality of the results \cite{comparing_compression}.
Our results tend to indicate the opposite, ref. Table~\ref{tab:compression_evaluation_results}.
The distance measures does not have a large impact on the quality of the results.
Though the compression algorithm does.
This difference may be caused by the difference of compression algorithm used between the two experiments.

For this text representation, the retained configuration is the BZip2 algorithm with the CBC distance measure.
Even though the LZMA algorithm give the best results, we decided to trade quality for time.
BZip2 produces relatively good results in a shorter amount of time ($\sim 5-6$ times faster).

\begin{table*}
  \centering
  \caption{Compression methods evaluation with different compression algorithm and distance metrics}
  \label{tab:compression_evaluation_results}

  \subcaption{Distance: NCD}
  \begin{tabular}{l c c c|c}
    \toprule
    AP/RPrec/HPrec & Oxquarry     & Brunet       & St-Jean       & Mean \\
    \midrule
    Bzip2          & 0.77/0.68/69 & 0.76/0.70/25 & 0.70/0.63/214 & 0.74/0.67/102\\
    GZip           & 0.62/0.56/41 & 0.61/0.53/24 & 0.45/0.44/054 & 0.56/0.51/040\\
    LZMA           & 0.81/0.70/82 & 0.78/0.73/27 & 0.71/0.63/241 & 0.76/0.68/117\\
    \bottomrule
  \end{tabular}

  \vspace{0.5cm}

  \subcaption{Distance: CBC}
  \begin{tabular}{l c c c|c}
    \toprule
    AP/RPrec/HPrec & Oxquarry     & Brunet       & St-Jean       & Mean\\
    \midrule
    Bzip2          & 0.79/0.69/74 & 0.76/0.70/25 & 0.70/0.62/219 & 0.75/0.67/106\\
    GZip           & 0.64/0.56/43 & 0.60/0.52/23 & 0.42/0.42/056 & 0.55/0.50/041\\
    LZMA           & 0.84/0.73/85 & 0.79/0.73/31 & 0.71/0.62/214 & 0.78/0.69/110\\
    \bottomrule
  \end{tabular}

\end{table*}

\begin{table}
  \centering
  \caption{Compression methods time evaluation with different compression algorithm and distance metrics}
  \label{tab:compression_evaluation_times}

  \subcaption{Distance: NCD}
  \begin{tabular}{l c c c|c}
    \toprule
    Time      & Oxquarry  & Brunet & St-Jean & Mean\\
    \midrule
    Bzip2     & 12.7s     & 8.4s   & 198.9s  & 73.3s\\
    GZip      & 15.0s     & 8.8s   & 211.3s  & 78.4s\\
    LZMA      & 69.0s     & 46.6s  & 1046.3s & 387.3s\\
    \bottomrule
  \end{tabular}

  \vspace{0.5cm}

  \subcaption{Distance: CBC}
  \begin{tabular}{l c c c|c}
    \toprule
    Time      & Oxquarry & Brunet & St-Jean & Mean\\
    \midrule
    Bzip2     & 12.7s    & 8.4s   & 198.4s  & 73.2s\\
    GZip      & 14.9s    & 8.9s   & 214.5s  & 79.4s\\
    LZMA      & 68.8s    & 46.8s  & 1052.0s & 398.2s\\
    \bottomrule
  \end{tabular}
\end{table}
