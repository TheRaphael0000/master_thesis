\subsection{Compression-based distances \label{sec:compression_based_distances}}

This section covers another method to compute distances between documents based on file compression.
The main idea is to first compress two documents A, B then compress the concatenation of A and B, denoted AB.

Using the sizes after compression of A, B, AB and a compression distance measure, it is possible to compute a distance between A and B.

Commonly, for the compression, a lossless compression algorithm is used, one from the Lempel-Ziv family (GZIP), the block sorting family (BZip2) or the statistical family (PPM)~\cite{comparing_compression}.

This technique is based on the fact that compression algorithms tries to lower the Shannon entropy of a document, thus when compressing a document with a large Shannon entropy the compressed document should have a larger size after compression than a document with a small Shannon entropy.
When concatenating two documents that share many terms, the entropy of the concatenated document should be lower than if the two documents present distinct vocabulary.

This approach has the benefit to produce rank lists in a nearly parameterless manner, only a distance metric and compression algorithm is needed.
A main drawback with this technique is the fact that the results can not be properly explained compared to MF method.

In this study the lossless compression algorithm used are : GZip, BZip2, LZMA.
These algorithms are already implemented in the Python standard library, the programming language used for this study~\cite{python_standard_library}.
Each compression algorithm can be tweaked with different parameters, the default settings are used except for the compression level (trade-off compression time and compression size).
By setting the compression level to the maximal setting, this ensures that the produced file will have the lowest possible Shannon entropy reachable with this algorithm, thus providing the best possible approximation of distance when used in conjunction with the compression distances.
Definitions~\ref{def:compress1},~\ref{def:compress2} and \ref{def:compress3} are the most popular compression distance measure~\cite{comparing_compression}~\cite{savoy_stylo}.

\begin{definition}[Conditional complexity of compression~\cite{comparing_compression}~\cite{savoy_stylo}\label{def:compress1}]
  The conditional complexity of compression of two documents A and B is computed as follows:
  \begin{equation}
    CCC(A, B) = C(AB) - C(A)
  \end{equation}
  Where C(AB) is the size after compression of the concatenation of A and B, and C(A) the size after compression of A.
  This metrics is not easy to use since the order of magnitude is not bounded and can depend a lot on the text sizes.
  The next ones try to mitigate this problem.
\end{definition}

\begin{definition}[Normalized compression distance~\cite{comparing_compression}~\cite{savoy_stylo}\label{def:compress2}]
  The normalized compression distance of two documents A and B as follows:
  \begin{equation}
    NCD(A, B) = \frac{C(AB) - \min(C(A), C(B))}{\max(C(A), C(B))}
  \end{equation}
  Where C(AB) is the size after compression of the concatenation of A and B, C(A) the size after compression of A and C(B) the size after compression of B.
  This metric gives a value in the range $\left[0, 1+\epsilon\right]$, with $\epsilon$ being a small positive value created by the imperfection of compression algorithms.
\end{definition}

\begin{definition}[Compression-based cosine~\cite{comparing_compression}~\cite{savoy_stylo}\label{def:compress3}]
  The compression-based cosine of two documents A and B is computed as follows:
  \begin{equation}
    CBC(A, B) = 1 - \frac{C(A) + C(B) - C(AB)}{\sqrt{C(A) \cdot C(B)}}
  \end{equation}
  Where C(AB) is the size after compression of the concatenation of A and B, C(A) the size after compression of A and C(B) the size after compression of B.
  This metric has the same properties as the cosine distance (Definition~\ref{def:cosine_dist}).
\end{definition}


\subsubsection{Evaluation}

This experiment try to compare the three proposed compression algorithm (GZip, BZip2, LZMA) for the compression based distance ranking.
For each algorithm and for each document, the size after compression is computed, as well as the concatenation of every document pairs.
Using these sizes and the NCD or CBC distance metrics (ref. Section~\ref{sec:compression_based_distances}), the rank list is evaluated.
This experiment is run on the three corpora three times to have a better approximation of the run time.
The results in terms of efficiency of the resulting rank list are shown in Table~\ref{tab:compression_evaluation_results}.
The average time of the three runs are in Table~\ref{tab:compression_evaluation_times} when run on an Intel(R) Core(TM) i7-5820K CPU @ 3.30GHz.

GZip seem to be giving the worse results on every dataset with an absolute average AP of $\sim -0.20$ in compared to LZMA and BZip2.
LZMA gives the best results on every dataset tested, but BZip2 have close results.

The Cosine-based compression distance (CBC) tend to give better results over the normalized compression distance (NCD).
In terms of time complexity BZip2 is the fastest algorithm of the 3 proposed, LZMA is $\sim ~5-6$ times slower than BZip2, and GZip slower than BZip2 by around $\sim 5-20$\%.
No significant time differences is recorded between the NCD and CBC distance measures, since the greatest complexity reside in the compression algorithm.
Even though LZMA algorithm give the best results, the configuration retained for the compression based distance is the BZip2 with the Cosine based compression distance, since it gives relative good results in a short amount of time.
Previous study show that the distance measure choice was more impactful than the choice of the compression algorithm in the regarding the quality of the results~\cite{comparing_compression}.
Our results tend to indicate the opposite, ref. Table~\ref{tab:compression_evaluation_results}.
The distance measures does not impact a lot the quality of the results, but the compression algorithm does.
This difference may be cause by the difference of compression algorithm used.

For this text representation, the retained configuration is the BZip2 algorithm with the CBC distance measure.
The Bzip2 was selected even though LZMA give slightly better results, due to the huge difference in run time.

\begin{table}
  \centering
  \caption{Evaluation of the different compression algorithm and distance metrics using Average Precision (AP), R-Precision (RP) and High Precision (HP)}
  \label{tab:compression_evaluation_results}

  \subcaption{Oxquarry}
  \label{tab:compression_evaluation_oxquarry}
  \begin{tabular}{l c c}
    \toprule
    AP/RPrec/HPrec & NCD          & CBC \\
    \midrule
    Bzip2          & 0.77/0.68/69 & 0.79/0.69/74 \\
    GZip           & 0.62/0.56/41 & 0.64/0.56/43 \\
    LZMA           & 0.81/0.70/82 & 0.84/0.73/85 \\
    \bottomrule
  \end{tabular}

  \vspace{0.5cm}

  \subcaption{Brunet}
  \label{tab:compression_evaluation_brunet}
  \begin{tabular}{l c c}
    \toprule
    AP/RPrec/HPrec & NCD          & CBC \\
    \midrule
    Bzip2          & 0.76/0.70/25 & 0.76/0.70/25 \\
    GZip           & 0.61/0.53/24 & 0.60/0.52/23 \\
    LZMA           & 0.78/0.73/27 & 0.79/0.73/31 \\
    \bottomrule
  \end{tabular}

  \vspace{0.5cm}

  \subcaption{St-Jean}
  \label{tab:compression_evaluation_st_jean}
  \begin{tabular}{l c c}
    \toprule
    AP/RPrec/HPrec & NCD           & CBC \\
    \midrule
    Bzip2          & 0.70/0.63/214 & 0.70/0.62/219 \\
    GZip           & 0.45/0.44/54  & 0.42/0.42/56 \\
    LZMA           & 0.71/0.63/241 & 0.71/0.62/214 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}
  \centering
  \caption{Average run time for the rank list computation with the different compression algorithm and distance metrics}
  \label{tab:compression_evaluation_times}

  \subcaption{Oxquarry}
  \label{tab:compression_evaluation_time_oxquarry}
  \begin{tabular}{l c c}
    \toprule
    Time      & NCD   & CBC \\
    \midrule
    Bzip2     & 12.7s & 12.7s \\
    GZip      & 15.0s & 14.9s \\
    LZMA      & 69.0s & 68.8s \\
    \bottomrule
  \end{tabular}

  \vspace{0.5cm}

  \subcaption{Brunet}
  \label{tab:compression_evaluation_time_brunet}
  \begin{tabular}{l c c}
    \toprule
    Time      & NCD   & CBC \\
    \midrule
    Bzip2     & 8.4s & 8.4s \\
    GZip      & 8.8s & 8.9s \\
    LZMA      & 46.6s & 46.8s \\
    \bottomrule
  \end{tabular}

  \vspace{0.5cm}

  \subcaption{St-Jean}
  \label{tab:compression_evaluation_time_st_jean}
  \begin{tabular}{l c c}
    \toprule
    Time      & NCD    & CBC \\
    \midrule
    Bzip2     & 198.9s  & 198.4s \\
    GZip      & 211.3s  & 214.5s \\
    LZMA      & 1046.3s & 1052.0s \\
    \bottomrule
  \end{tabular}
\end{table}
