\subsection{MF Tokens and Lemmas}

No clear value of $n$ for the $n$-MF is to choose over others, but depending on the documents' length a value between $50$ and $500$ tends to produce good results for the word token text representation~\cite{savoy_text_representation}.
One of the main advantage of this representation is that once a distance is computed based on this vector.
The results can be easily explained, since the feature vector is basically the proportion of the most frequent words in a document.

Instead of using directly the words to create the feature vector, another possibility is to use the lemma corresponding to each word, for example the sentence \textit{i saw two men with a saw} its lemmatized version is : \textit{i see two man with a saw} this requires advanced text preprocessing, but it can remove ambiguity.
