\section{Most Frequent Approach}

The first method to represent and compare the documents is called: Most frequent (MF).
This method try to compare documents by creating feature vectors containing frequencies of the most frequent items in a corpus for a specific text representation (TR).
A text representation can be for example words in the text or any new representation generated from the text.
In the case where words are used, the most frequent items will be the most common words in a corpus.
This method is a generalized approach of the most frequent words technique (MFWs), which consider words as items.

By only considering the most frequent items, this ensures that no all the document details will be synthesized in the vector, but rather the most important in the document.
The vector will thus contain the style of the document rather than the topic of the document.
When comparing these vector, this method focus on the difference between the authors styles rather than the topic of the documents or the time period the text was written in.
This assumption is experimented in this study in Section~\ref{sec:influance_mf_restriction}.

Previous studies have shown the importance of having documents of good quality and with at least 5000 tokens to have reliable results.
Skilled authors can easily change their style to imitate others for small texts, but it becomes more difficult for larger texts~\cite{savoy_stylo}.
This problematic is explored in Section~\ref{sec:importance_of_text_size}.

To be able to express a document as a feature vector of size $n$, the strategy used is to find the $n$ most frequent ($n$-MF) items in a corpus.
For each document, compute the items' relative frequency on the $n$-MF items.
The items' occurrence vector is the number of times each item is present in a document text representation.
The items' relative frequency vector is obtained by normalizing the items' occurrence vector such that its $L_1$ norm is 1, see Definition~\ref{def:l_1}.

\begin{definition}[$L_1$ norm]
  \label{def:l_1}
  The $L_1$ norm for a vector $x$ correspond to the sum all its elements.
  \begin{gather*}
    \|x\|_{1} = \sum _{i=1}^{n} \left| x_{i} \right|
  \end{gather*}
\end{definition}

By normalizing the vector, this ensures that long and short texts are comparable.
The items' relative frequency vector will represent the MF items proportions contained in a document.

For example, if a corpus contains novels of multiple genres (e.g. Comedy, Sci-fi, Fantastic, Romance).
With the right $n$, most of the items specific to the genre will be discarded, since their frequency should be lower than the non topic related items which are contained in every document.
The frequencies of these items should reflect the style of the author.

In this study, multiple text representation are explored, such as tokens/lemmas (Section~\ref{sec:tokens_lemmas}), substrings (Section~\ref{sec:substrings}) and POS sequences (Section~\ref{sec:pos_sequences}).

Example~\ref{ex:mf_vector} show the computation of the MF items vector, using words as items.

\begin{example}
  \centering
  \caption{MF vector computation, example with tokens}
  \label{ex:mf_vector}

  \begin{subexample}{\linewidth}
    \subcaption{Document X}
    "\textit{i realize now , that i was not looking in . i was looking out . and he , on the other side was looking in .}"\cite{ddlc}
  \end{subexample}

  \vspace{0.5cm}

  \begin{subexample}{\linewidth}
  \subcaption{Vector}
    Suppose that the $5$-MF tokens for this text's \textbf{corpus} are "\textit{the, was, i, she, he}".

    The vector using the MF strategy in Document X is thus:
    \vspace{0.2cm}

    \centering
    \begin{tabular}{l c c c c c}
      \toprule
                       & the & was & i   & she & he  \\
      \midrule
      occurrences         & 1   & 3   & 3   & 0   & 1   \\
      relative frequency & 1/8 & 3/8 & 3/8 & 0/8 & 1/8 \\
      \bottomrule
    \end{tabular}
  \end{subexample}
\end{example}

\subsection{Normalization \label{sec:normalization}}

Applying a normalization can improve the performances of the distance metrics presented in Section~\ref{sec:vectors_distances}.
The Z-Score normalization is one of them, see Definition~\ref{def:z_score}.

In this study, the following distance metrics are always normalized using the Z-Score normalization: Manhattan, Euclidean and Cosine Distance, since they tend to produce better results when normalized.
The Manhattan distance using the Z-Score normalization on MF words vectors is also called Delta model~\cite{savoy_stylo}.

Definition~\ref{def:normalization} show two ways to linearly normalize a vector.
These are used for to fit data to distributions which require a certain interval.
In this study, linear normalization is used to fit the beta distribution.

\begin{definition}[Z-Score normalization~\cite{savoy_stylo}]
  \label{def:z_score}
  \begin{gather*}
    ZScore(X) = \frac{X - \mu}{\sigma}
  \end{gather*}
  Z-Score normalize a vector X, such that the resulting vector have a $0$ mean and a standard deviation of $1$.
  When using the Z-Score normalization on MF vectors, $\mu$ and $\sigma$ usually are vectors containing the mean and the standard deviation of each item in the corpus.
\end{definition}

\begin{definition}[Linear normalization]
  \label{def:normalization}
  \begin{gather*}
    Norm(X) = \frac{X - Min(X)}{Max(X) - Min(X)}
  \end{gather*}
  Normalize a vector X in the interval $[0, 1]$.
  \begin{gather*}
    Norm_{ab}(X, a, b) = Norm(X) \cdot (b - a) + a
  \end{gather*}
  Normalize a vector X in the interval $[a, b]$, with $b > a$.
\end{definition}

\subsection{Smoothing}

Relative item frequencies can be considered as a probability of occurrence based on the maximum likelihood principle.
The main problem with this approach is that the probability of occurrence of frequent items tend to be overestimated and underestimate the probability of occurrence of low frequency items or for items that never occur.
For example, if an item is not present in a document its relative item frequency is 0, but this should not mean that the probability of the author using this item is 0.

The solution proposed is to use a smoothing technique such as the Lidstone smoothing presented in Definition~\ref{def:lidstone_smoothing}.
Smoothing techniques can help distance functions based on probabilities, such as the Kullback-Leibler Divergence~\cite{savoy_stylo}.

In this study, when a smoothing technique is used, except stated otherwise, the Lidstone smoothing is used with $\lambda = 0.1$ since it tends to produce better results~\cite{savoy_stylo}.

\begin{definition}[Lidstone smoothing~\cite{savoy_stylo}]
  \label{def:lidstone_smoothing}
  \begin{gather*}
    p(t_i, D_j) = \frac{tf_{i,j} + \lambda}{n + \lambda \cdot |V|}
  \end{gather*}
  With $t_i$ the i-th item, $D_j$ the j-th document, $tf_{i,j}$ the number of occurrences of the i-th item in $D_j$, $|V|$ the size of the vocabulary, $\lambda$ a small value ($\lambda = 1$, is a special case called Laplace smoothing), n the total number of items in the document $D_j$.
\end{definition}
