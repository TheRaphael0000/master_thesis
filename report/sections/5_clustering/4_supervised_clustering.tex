\subsection{Supervised Clustering}

\subsubsection{Method}

To learn at which position in the rank list the cut should be, this third idea is to fit a linear regression model on samples created from the rank list.
The model must be linear since only one cut should seperate the true links and the false links.
To train the model, a sample is created for each link in a rank list.
The links labels are either \textit{true} (1.0) when both document in the link are from the same author and \textit{false} (0.0) otherwise.

The features used are : the log of the relative rank ($log \frac{rank}{|L|}$) and the score of the link.
The first feature to take into account that true links are generally on the top of the rank list.
The value is normalized to be able to generalize this value to any rank list size.
The logarithm allow to have a greater contrast in value for the top rank while considering bottom rank as more or less equal.
The second feature aim to consider that the small distances are generally true links.

Using these two features, the model can grasp the importance of the rank and the score of each link.
Since the training is only based on the relative rank and the score at each rank, the trained model is language independent and size independent.
But this model is metric dependent, since the score magnitude can variate depending on the distance function.

In this study, the model used is the logistic regression.
The advantage of using a regression model is that the output of the model will correspond to a probability of being a true link according to the model.
To find the cut on the test datasets, the fitted model predict the probability of being a true link on every link in the new rank list.
From these predictions, a probability threshold must be chosen.

For example, having a probability threshold at $0.5$, minimize both false negatives and the false positives.
This can be adjusted, for example if false negatives are more important to minimize, a probability threshold at $0.6$ can be selected instead or $0.4$ if the false positive should be minimized.
For the sake of simplicity, the probability threshold chosen is $0.5$.

The distance threshold to correctly separate true links to false links is chosen by linearly interpolating the probability threshold with its closest probability of the one above and the one below to their scores.
This ensures that the distance threshold chosen is even correct between ranks for any new corpus.
Example~\ref{ex:linear_interpolation} showcase a linear interpolation computation using a probability threshold of $0.5$.
The logistic regression model can be re-used on any other rank lists produced with the same distance metrics.

\begin{example}
  \centering
  \caption{Linear interpolation for supervised distance threshold selection (probability threshold fixed at 0.5)}
  \label{ex:linear_interpolation}

  \begin{subexample}{\linewidth}
    \centering
    \subcaption{Rank list with link probability and score}
    \begin{tabular}{l r r}
      \toprule
      Rank & Probability & Score \\
      \midrule
      (...) & &\\
      45th & 0.54 & 15 \\
      46th & 0.52 & 13 \\
      47th & 0.49 & 12 \\
      48th & 0.48 & 10 \\
      (...) & & \\
      \bottomrule
    \end{tabular}
  \end{subexample}

  \vspace{0.5cm}

  \begin{subexample}{\linewidth}
    \centering
    \subcaption{Linear interpolation}
    \begin{align}
        \alpha &= \frac{0.5 - 0.49}{0.52 - 0.49} = \frac{1}{3} \\
        \textit{distance\_threshold}_{@0.5} &= (13 - 12) \cdot \alpha + 12 = 12 + \frac{1}{3}
    \end{align}
  \end{subexample}
\end{example}


\subsubsection{Evaluation}

To evaluate the supervised clustering approach, every dataset were used.
First a rank list for each dataset is computed using a Z-Score fusion using the retained rank list, see Section~\ref{sec:annex_retained_text_representation} in annex.
Then every pair of rank list is used to create a training/testing evaluation.
In other words, a model to create cuts is trained on every dataset and tested on all the datasets.

The model does not depend on the rank list size since only relative magnitudes are used.
The BCubed metrics and the r ratio difference are computed during this experiment and are in presented in Table~\ref{tab:supervised_clustering_train_test}, for every pairs of dataset.
The average values obtained for each metrics is presented in Table~\ref{tab:supervised_clustering_average}.

Two conclusion can be drawn with these results:
\begin{itemize}
  \item
  When testing the cut model, having a rank list of good quality tends to produce better clustering, no matter the quality of the rank list used for the training.
  When testing the clustering model on St-Jean B (the corpus with the best rank list obtained, 0.96 AP), it obtains in average the best $B^3_{F_1}$ 0.93 for any training model.
  The least performing rank list also have the least accurate results during the clustering task, Brunet with 0.76 AP have an average clustering $B^3_{F_1}$ of 0.80.
  \item
  Using the best rank list for training the cut model does not always give the best results.
  An example to illustrate this observation, the St-Jean B dataset have the best rank list with an average precision of 0.96, but the best rank list for training the clustering model is Oxquarry with an average $r_{diff}$ of 0.02 and an average $B^3_{F_1}$ of 0.87.
  In the other hand, St-Jean B have an average $r_{diff}$ of 0.09 and an average $B^3_{F_1}$ of 0.83.
  With the corpora used, the Oxquarry rank list seem to be the best corpus to train this type of model and Brunet the least effective.
\end{itemize}

The conclusion to the two previous points is that having a good rank list is more important for testing than training.
No clear rank lists property was found which makes some of them better to train the model.

\begin{table*}
  \centering
  \caption{Supervised clustering evaluation}
  \label{tab:supervised_clustering}

  \subcaption{Every possible threshold and dataset pairs. Metrics in order : $B^{3}_{F_1}$ / $B^{3}_{precision}$ / $B^{3}_{recall}$ / $r_{diff}$}
  \label{tab:supervised_clustering_train_test}
  \begin{tabular}{l l| c c c c}
    \toprule
    \multicolumn{2}{c}{\multirow{2}{*}{}} & \multicolumn{4}{c}{Testing} \\
    \multicolumn{2}{c}{} & Oxquarry & Brunet & St-Jean A & St-Jean B \\
    \midrule
    \parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{Training}}}
    & Oxquarry
    & 0.82/1.00/0.70/0.08
    & 0.82/0.88/0.77/0.07
    & 0.87/0.84/0.89/-0.02
    & 0.95/0.92/0.98/-0.01
    \\
    & Brunet
    & 0.80/1.00/0.67/0.10
    & 0.75/0.94/0.62/0.18
    & 0.82/0.96/0.72/0.07
    & 0.91/1.00/0.83/0.05
    \\
    & St-Jean A
    & 0.80/1.00/0.67/0.10
    & 0.82/0.94/0.73/0.11
    & 0.84/0.93/0.76/0.04
    & 0.95/1.00/0.91/0.03
    \\
    & St-Jean B
    & 0.80/1.00/0.67/0.10
    & 0.76/0.94/0.64/0.16
    & 0.83/0.93/0.74/0.05
    & 0.95/1.00/0.91/0.03
    \\
    \bottomrule
  \end{tabular}

  \vspace{0.5cm}

  \subcaption{Metrics averages}
  \label{tab:supervised_clustering_average}
  \begin{tabular}{l c c c c}
    \toprule
    & $B^{3}_{F_1}$
    & $B^{3}_{precision}$
    & $B^{3}_{recall}$
    & $r_{diff}$ \\
    \midrule
    Testing on Oxquarry     & 0.81 & 1.00 & 0.68 & 0.09 \\
    Testing on Brunet       & 0.79 & 0.92 & 0.69 & 0.13 \\
    Testing on St-Jean A    & 0.85 & 0.92 & 0.78 & 0.04 \\
    Testing on St-Jean B    & 0.94 & 0.98 & 0.91 & 0.03 \\
    Training on Oxquarry    & 0.86 & 0.91 & 0.84 & 0.04 \\
    Training on Brunet      & 0.82 & 0.97 & 0.71 & 0.10 \\
    Training on St-Jean A   & 0.85 & 0.97 & 0.77 & 0.07 \\
    Training on St-Jean B   & 0.84 & 0.97 & 0.74 & 0.08 \\
    \textbf{Global absolute mean} & \textbf{0.84} & \textbf{0.95} & \textbf{0.76} & \textbf{0.07} \\
    \bottomrule
  \end{tabular}
\end{table*}
