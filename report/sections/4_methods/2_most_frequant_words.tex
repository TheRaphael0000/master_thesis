\subsection{Most frequent words (MFW), feature vectors}

The first method used to represent and compare the documents is called: Most frequant words (acronym: MFW), and try to compare documents by creating feature vectors containing words frequencies for the most common words.
To be able to express a document as a feature vector of size $n$, a common strategy in Stylometry is to find the $n$ most frequent words ($n$-MFW) in a corpus, and for each document compute their word relative frequency by dividing by the total number of $n$-MFW in the document~\cite{savoy_stylo}.
The resulting vector will represent the MFW proportions contained in a document.
Example~\ref{ex:mfw_vector} show for the MFW vector computation.

\begin{example}
  \caption{MFW vector computation}
  \label{ex:mfw_vector}

  \begin{subexample}
    \subcaption{Text}
    "\textit{i realize now that i was not looking in i was looking out and he on the other side was looking in}"\cite{ddlc}
    Suppose that the corpus in which this text belong the $5$-MFW are "\textit{the, was, i, she, he}".
  \end{subexample}

  \begin{subexample}
    \subcaption{Vector}
    \centering
    \begin{tabular}{l c c c c c}
      \toprule
                         & the & was & i   & she & he  \\
      \midrule
      occurrences         & 1   & 3   & 3   & 0   & 1   \\
      relative frequency & 1/8 & 3/8 & 3/8 & 0/8 & 1/8 \\
      \bottomrule
    \end{tabular}
  \end{subexample}
\end{example}

No clear value of $n$ for the $n$-MFW is to choose over others, but depending on the documents' length a value between $50$ and $500$ tends to produce good results for the word token text representation~\cite{savoy_text_representation}.
One of the main advantage of this reprentation is that once a distance is computed based on this vector. The results can be easily explained, since the feature vector is basicaly the proportion of the most frequent words in a document.

Instead of using directly the words to create the feature vector, another possibility is to use the lemma corresponding to each word, for example the sentence \textit{i saw two men with a saw} its lemmatized version is : \textit{i see two man with a saw} this requires advanced text preprocessing, but it can remove ambiguity.

Instead of considering every token as a word, another popular method is to create a "word" from a short sequences of letters called Letters $n$-grams using Definition~\ref{def:letters_n_grams}.
To consider overlapping words, the whole document is synthesized into a long string by joining every token by an \textit{underscore} : $\_$, to remove visual ambiguity due to the nature of the space character.
This technique can be further explored by combining for example two or more $n$-grams sizes together.
$n$-grams can be really effective even though no clear meaning from each $n$-grams can be exploited at first sight.
A simple explanation on why $n$-grams can be effective is by using $n$-grams of size 3 to 5 in most Germanic languages this can correspond to for example : the tense of the verbs (i.e. : \textit{ing\_}, \textit{ed\_}), some small common words (\textit{\_the\_}, \textit{\_and\_}, \textit{\_that}), or in French the adverbs (\textit{\_ment}) which can capture the style of an author.

\begin{definition}[Letters $n$-grams]
  \label{def:letters_n_grams}
  A letter $n$-grams is a special type of tokenization which is constructed by creating a token of size $n$ for each substrings starting from the position $0$ to \textit{text\_size} $- n - 1$.
  Example: Using 3-grams The string: \textit{"fox\_is\_brown"} is converted to: (\textit{"fox", "ox\_", "x\_i", "\_is", "is\_", "s\_b", "\_br", "bro", "row", "own"})
\end{definition}

An alternative version of the $n$-grams algorithm used here is defined in Definition~\ref{def:words_n_grams}.
This algorithm considers each token as a string to apply the Letters $n$-grams algorithm to.
To differenciate it from the classical $n$-grams algorithm,
The latter algorithm, is called in this study In-word $n$-grams (Definition~\ref{def:words_n_grams}) and the other one is called Letters $n$-grams (Definition~\ref{def:letters_n_grams}).

\begin{definition}[In-words $n$-grams]
  \label{def:words_n_grams}
  In-word $n$-grams are created by applying the Letters $n$-grams algorithm on each token.
  When a token is smaller than $n$, the whole token is considered.
  Example: Using words 3-grams on the following tokens: ["fox", "is", "brown"] is converted to: (\textit{"fox", "is", "bro", "row, "own"})
\end{definition}

Another possible stylistic aspect that can be detected from a text using the MFW approach is to consider the sentence constructions.
This can be solved by creating short sequences ($n$-grams) of POS tags.
In this case, each POS is considered as a character in the Letters $n$-grams definition, this type of $n$-grams is also known as w-shingling.
For example, the sentence : \textit{"The cat eat a fish"} has the following POS tag \textit{"Article Noun Verb Article Noun"} which correspond to the following 3-grams of POS : \textit{"Article Noun Verb"} / \textit{"Noun Verb Article"} / \textit{"Verb Article Noun"}.
In practice the POS is more detailed, for example instead of just considering \textit{eat} as a verb, a more detailed POS can be the verb and its tense \textit{Verb-SimplePresent}, the same goes for the other type of POS.

When using features vectors based on the MFW method, once each document is represented as a feature vector they can be compared using metric in Section~\ref{sec:fv_distances}.

\subsubsection{Vectors distances \label{sec:fv_distances}}

To be able to compare vector, different metrics can be used depending on the usage.
Usually two types of metrics for vector comparison can be used: Vector similarities and Vector distances, the first yield a large value when the two vectors are closely related and the second a small value when they are similar.
The following definitions are one of the few most common $L^1$/$L^2$/$cosine$ based distances.

\begin{definition}[Manhattan distance ($L^1$ based)]
  \begin{equation}
    dist_{Manhattan}(A, B) = \sum_{i=1}^{m} |A_i - B_i|
  \end{equation}
  Example:

  $A = \left[-2, 3, -5, 7, -11, 13 \right]$

  $B = \left[1, 2, 4, 8, 16, 32 \right]$
  \begin{equation*}
    \begin{aligned}
      dist_{Manhattan}(A, B) =& |-2 - 1| + |3 - 2| + |-5 - 4| \\
                             +& |7 - 8| + |-11 - 16| + |13 - 32| \\
                             =& |-3| + |1| + |-9| \\
                             +& |-1| + |-27| + |-19| \\
                             =& 60
    \end{aligned}
  \end{equation*}
\end{definition}

\begin{definition}[Tanimoto distance ($L^1$ based, normalized)]
  \begin{equation}
    dist_{Tanimoto}(A, B) = \frac{dist_{Manhattan}(A, B)}{\sum_{i=1}^{m} max(A_i, B_i)}
  \end{equation}
  It's a components-wised normalized version of the manhattan distance.
\end{definition}

\begin{definition}[Euclidean distance ($L^2$ based)]
  \begin{equation}
    dist_{Euclidian}(A, B) = \sqrt{\sum_{i=0}^{m}(A_i - B_i)^2}
  \end{equation}
\end{definition}

\begin{definition}[Matusita distance ($L^2$ based, square rooted values)]
  \begin{equation}
    \begin{split}
      dist_{Matusita}(A, B) &= dist_{Euclidian}(A', B') \\
      \text{with }A' &= \sqrt{A}\text{ and }B' = \sqrt{B}
    \end{split}
  \end{equation}
\end{definition}

\begin{definition}[Clark distance ($L^2$ based)~\cite{kocher_verification}]
  \begin{equation}
    dist_{Clark}(A, B) = \sqrt{\sum_{i=0}^{m}\left(\frac{|A_i - B_i|}{A_i + B_i}\right)^2}
  \end{equation}
\end{definition}

\begin{definition}[Cosine distance (Inner product based)]
  \label{def:cosine_dist}
  To compute the Cosine distance, first the cosine similarity must be computed.
  \begin{equation}
    sim_{Cosine}(A, B) = \frac{\langle A, B \rangle}{\sqrt{\langle A , A \rangle}\sqrt{\langle B, B \rangle}}
  \end{equation}
  The cosine similarity is ranged between -1 and 1.
  With 1 beeing exactly the same, -1 the total opposite and 0 orthogonal.
  \begin{equation}
    dist_{Cosine}(A, B) = 1 - sim_{Cosine}(A, B)
  \end{equation}
  \begin{equation}
    dist_{Angular\_cosine}(A, B) = \frac{cos^{-1}\left( sim_{Cosine}(A, B) \right)}{\pi}
  \end{equation}
\end{definition}

\begin{definition}[Kullback-Leibler divergence (relative entropy)]
  \begin{equation}
    dist_{kld}(A, B) = \sum_{i=0}^{m} A_i \cdot \log(\frac{A_i}{B_i})
  \end{equation}
\end{definition}

\begin{definition}[Jeffrey divergence~\cite{kocher_verification}]
  \begin{equation}
    dist_{j\_divergence}(A, B) = \sum_{i=0}^{m} (A_i - B_i) \cdot \log(\frac{A_i}{B_i})
  \end{equation}
  It represents the difference between two probability distribution.
\end{definition}


\subsubsection{Normalization}

Sometimes when vectors deal with different order of magnitude, they can not be easily compared with some previously cited distance metrics.
Thus using a normalization can improve the performances.

\begin{definition}[Z-score normalization~\cite{savoy_stylo}]
  \label{def:z_score}
  \begin{equation}
    Zscore(X) = \frac{X - \mu}{\sigma}
  \end{equation}
  Zscore normalize X a list of numbers such that the resulting list have a 0 mean and a standard deviation of 1.
  When using the z-score normalization on MFW vectors, $\mu$ and $\sigma$ usually are vectors containing the mean and the standard deviation of each terms in the corpus.
\end{definition}

The Manhattan distance applied on a Z-score normalized MFW vector is also called Burrows's Delta~\cite{savoy_stylo}.
In this study, the following distance metrics are always normalized using the Z-score normalization before hand : Manhattan, Euclidean and Cosine Distance, since they tends to produce better results when normalized.

\begin{definition}[Linear normalization]
  \label{def:normalization}
  \begin{equation}
    Norm(X) = \frac{X - Min(X)}{Max(X) - Min(X)}
  \end{equation}
  Normalize a list of numbers X in the interval $[0, 1]$.

  \begin{equation}
    Norm_{ab}(X, a, b) = Norm(X) \cdot (b - a) + a
  \end{equation}
  Normalize a list of numbers X in the interval $[a, b]$, with $b > a$.
\end{definition}

Definition~\ref{def:normalization} show two simple ways to linearly normalize a list of numbers.

\subsubsection{Smoothing}

Relative term frequencies can be considered as a probability of occurrence by using the maximum likelihood principle.
The main problem with this approach is that the probability of occurrence of frequent terms tend to be overhestimated and underestimate the probability of occurrence of low frequency terms.
If for example, a term is not present in a document its relative term frequency is 0, but this should not mean that the probability of the author using this word is 0.
The solution proposed is to use a smoothing technique such as the Lidstone smoothing presented in Definition~\ref{def:lidstone_smoothing}.
Smoothing techniques can help distance functions based on probabilities, such as the Kullback-Leibler Divergence.~\cite{savoy_stylo}

\begin{definition}[Lidstone smoothing~\cite{savoy_stylo}]
  \label{def:lidstone_smoothing}
  \begin{equation}
    p(t_i, D_j) = \frac{tf_{i,j} + \lambda}{n + \lambda \cdot |V|}
  \end{equation}
  With $t_i$ the i-th term, $D_j$ the j-th document, $tf_{i,j}$ the number of occurrence of the i-th term in $D_j$, $|V|$ the size of the vocabulary, $\lambda$ a small value ($\lambda = 1$ special, case called Laplace smoothing, but generally a smaller value such as 0.1 give good results), n the total number of words in the document $D_j$.
\end{definition}

In this study, when a smoothing technique is used, except stated otherwise, the Lidstone smoothing is used with $\lambda = 0.1$ since it tends to produce better results~\cite{savoy_stylo}.
